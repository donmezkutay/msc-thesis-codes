{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "848c998c-b686-425a-83ae-bd2563d12a42",
   "metadata": {},
   "source": [
    "## 0. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e09a77e1-6622-4ab7-89a9-ad77424052a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import climextremes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa6d6a4-d5da-44b3-8bff-893f06c70795",
   "metadata": {},
   "source": [
    "## 1. Open Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd97c8cb-9a0b-449b-92a9-838f20f2cd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = climextremes.Fort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2a3d7d0-da6e-40d0-8663-0c78fcd684d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obs</th>\n",
       "      <th>tobs</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>year</th>\n",
       "      <th>Prec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36520</th>\n",
       "      <td>36520.0</td>\n",
       "      <td>361.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36521</th>\n",
       "      <td>36521.0</td>\n",
       "      <td>362.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36522</th>\n",
       "      <td>36522.0</td>\n",
       "      <td>363.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36523</th>\n",
       "      <td>36523.0</td>\n",
       "      <td>364.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36524</th>\n",
       "      <td>36524.0</td>\n",
       "      <td>365.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36524 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           obs   tobs  month   day    year  Prec\n",
       "1          1.0    1.0    1.0   1.0  1900.0   0.0\n",
       "2          2.0    2.0    1.0   2.0  1900.0   0.0\n",
       "3          3.0    3.0    1.0   3.0  1900.0   0.0\n",
       "4          4.0    4.0    1.0   4.0  1900.0   0.0\n",
       "5          5.0    5.0    1.0   5.0  1900.0   0.0\n",
       "...        ...    ...    ...   ...     ...   ...\n",
       "36520  36520.0  361.0   12.0  27.0  1999.0   0.0\n",
       "36521  36521.0  362.0   12.0  28.0  1999.0   0.0\n",
       "36522  36522.0  363.0   12.0  29.0  1999.0   0.0\n",
       "36523  36523.0  364.0   12.0  30.0  1999.0   0.0\n",
       "36524  36524.0  365.0   12.0  31.0  1999.0   0.0\n",
       "\n",
       "[36524 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68c3123b-f0f8-4966-a43d-9576f8aa0c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prec</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1900.0</th>\n",
       "      <td>2.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1901.0</th>\n",
       "      <td>2.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1902.0</th>\n",
       "      <td>4.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1903.0</th>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1904.0</th>\n",
       "      <td>3.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995.0</th>\n",
       "      <td>1.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996.0</th>\n",
       "      <td>1.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997.0</th>\n",
       "      <td>4.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998.0</th>\n",
       "      <td>1.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999.0</th>\n",
       "      <td>2.41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Prec\n",
       "year        \n",
       "1900.0  2.39\n",
       "1901.0  2.32\n",
       "1902.0  4.34\n",
       "1903.0  0.85\n",
       "1904.0  3.02\n",
       "...      ...\n",
       "1995.0  1.52\n",
       "1996.0  1.35\n",
       "1997.0  4.63\n",
       "1998.0  1.83\n",
       "1999.0  2.41\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtMax = dt.groupby('year').max()[['Prec']]\n",
    "dtMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e429790-8ec2-48e8-90df-5351c088a888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.39, 2.32, 4.34, 0.85, 3.02, 1.74, 1.7 , 1.21, 1.93, 1.68, 1.48,\n",
       "       1.16, 1.32, 1.32, 1.44, 1.83, 2.23, 0.96, 2.98, 0.97, 1.16, 1.46,\n",
       "       0.84, 2.3 , 1.38, 1.7 , 1.17, 1.15, 1.32, 1.25, 1.56, 1.24, 1.89,\n",
       "       1.93, 0.71, 1.76, 1.05, 0.93, 3.54, 0.6 , 1.51, 1.6 , 2.19, 1.42,\n",
       "       1.17, 0.87, 2.23, 2.15, 1.08, 3.54, 2.13, 3.06, 1.69, 1.84, 0.71,\n",
       "       0.98, 0.96, 2.18, 1.76, 1.21, 1.61, 3.21, 1.02, 2.69, 0.98, 2.71,\n",
       "       0.95, 2.12, 1.51, 1.36, 2.4 , 1.62, 0.71, 1.1 , 2.85, 2.15, 1.03,\n",
       "       4.43, 1.85, 1.99, 1.15, 1.34, 2.97, 1.87, 2.03, 1.46, 0.94, 1.29,\n",
       "       1.62, 1.12, 3.48, 0.95, 2.49, 1.03, 1.81, 1.52, 1.35, 4.63, 1.83,\n",
       "       2.41])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(dtMax.Prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d8f1282-a859-4568-a846-fcca892596b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.39, 2.32, 4.34, 0.85, 3.02, 1.74, 1.7 , 1.21, 1.93, 1.68, 1.48,\n",
       "       1.16, 1.32, 1.32, 1.44, 1.83, 2.23, 0.96, 2.98, 0.97, 1.16, 1.46,\n",
       "       0.84, 2.3 , 1.38, 1.7 , 1.17, 1.15, 1.32, 1.25, 1.56, 1.24, 1.89,\n",
       "       1.93, 0.71, 1.76, 1.05, 0.93, 3.54, 0.6 , 1.51, 1.6 , 2.19, 1.42,\n",
       "       1.17, 0.87, 2.23, 2.15, 1.08, 3.54, 2.13, 3.06, 1.69, 1.84, 0.71,\n",
       "       0.98, 0.96, 2.18, 1.76, 1.21, 1.61, 3.21, 1.02, 2.69, 0.98, 2.71,\n",
       "       0.95, 2.12, 1.51, 1.36, 2.4 , 1.62, 0.71, 1.1 , 2.85, 2.15, 1.03,\n",
       "       4.43, 1.85, 1.99, 1.15, 1.34, 2.97, 1.87, 2.03, 1.46, 0.94, 1.29,\n",
       "       1.62, 1.12, 3.48, 0.95, 2.49, 1.03, 1.81, 1.52, 1.35, 4.63, 1.83,\n",
       "       2.41, 2.39, 2.32, 4.34, 0.85, 3.02, 1.74, 1.7 , 1.21, 1.93, 1.68,\n",
       "       1.48, 1.16, 1.32, 1.32, 1.44, 1.83, 2.23, 0.96, 2.98, 0.97, 1.16,\n",
       "       1.46, 0.84, 2.3 , 1.38, 1.7 , 1.17, 1.15, 1.32, 1.25, 1.56, 1.24,\n",
       "       1.89, 1.93, 0.71, 1.76, 1.05, 0.93, 3.54, 0.6 , 1.51, 1.6 , 2.19,\n",
       "       1.42, 1.17, 0.87, 2.23, 2.15, 1.08, 3.54, 2.13, 3.06, 1.69, 1.84,\n",
       "       0.71, 0.98, 0.96, 2.18, 1.76, 1.21, 1.61, 3.21, 1.02, 2.69, 0.98,\n",
       "       2.71, 0.95, 2.12, 1.51, 1.36, 2.4 , 1.62, 0.71, 1.1 , 2.85, 2.15,\n",
       "       1.03, 4.43, 1.85, 1.99, 1.15, 1.34, 2.97, 1.87, 2.03, 1.46, 0.94,\n",
       "       1.29, 1.62, 1.12, 3.48, 0.95, 2.49, 1.03, 1.81, 1.52, 1.35, 4.63,\n",
       "       1.83, 2.41])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.append(np.array(dtMax.Prec), np.array(dtMax.Prec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1413dc14-24ec-42f1-9e51-afafba83a296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    36524.000000\n",
       "mean         0.041814\n",
       "std          0.166767\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          0.000000\n",
       "75%          0.000000\n",
       "max          4.630000\n",
       "Name: Prec, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt['Prec'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23a44dc-e5f9-40e5-bad5-dbeed02719b0",
   "metadata": {},
   "source": [
    "## 3. Basic Explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311a5fe4-b166-4d3c-8a3f-8e05fe757315",
   "metadata": {},
   "source": [
    "**single block:** for Block Maximum approach think single block as a *one-year* (since there are yearly extreme values and a single block corresponds to each of these years). The definition for POT approaches can differ (i.e., monthly instead of yearly)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeee8951-a455-42a6-8aa8-523b179b5a68",
   "metadata": {},
   "source": [
    "**The return probability** is the probability of exceeding the return value in a single block."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09498689-1192-434b-b0e0-4f69c6e37107",
   "metadata": {},
   "source": [
    "**The return period** is the average number of blocks expected to occur before the return value is exceeded and is equal to the inverse of **the probability of exceeding** the return value in a single block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae0fb7c-439c-4cd5-b0a5-31be5010d1e6",
   "metadata": {},
   "source": [
    "**The return value** is the value for which the expected number of blocks until an event that exceeds that value is equal to the **return period**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500eef6c-5e54-4c6a-b8b2-ebe938443b0e",
   "metadata": {},
   "source": [
    "**asymptotic distribution:** an asymptotic distribution exists if the probability distribution of Zi converges to a probability distribution (the asymptotic distribution) as i increases: see convergence in distribution. Read asymptotics as “what happens to the thing I’m estimating as my sample gets big? Approaching to the true distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f385a9-83f1-4428-95b9-7b7e973b6b48",
   "metadata": {},
   "source": [
    "In simple terms, any statistic can be a **point estimate**. A statistic is an estimator of some parameter in a population. For example:\n",
    "\n",
    "- The sample standard deviation (s) is a *point estimate* of the population standard deviation (σ).\n",
    "- The sample mean (̄x) is a *point estimate* of the population mean, μ.\n",
    "- The sample variance (s2) is a *point estimate* of the population variance (σ2).\n",
    "\n",
    "In more formal terms, the estimate occurs as a result of point estimation applied to a set of sample data. Points are single values, in comparison to **interval estimates**, which are a range of values. For example, a confidence interval is one example of an interval estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1e8610-8bbd-4c5a-82c9-64defd2e444e",
   "metadata": {},
   "source": [
    "**The delta method** is a general method for deriving the variance of a function of asymptotically normal random variables with known variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff153ae-fb50-49c0-b7c3-4b63d6e0b513",
   "metadata": {},
   "source": [
    "There are a number of ways to compute the standard errors for the margins of a regression. It might be possible to derive a probability density function for the margin itself, but that’s perhaps a huge pain and might not even exist. It is also possible to use simulation or **bootstrapping** to create **standard errors** for the margin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9071fa7-a70a-40d3-a2b9-c44f45c18d96",
   "metadata": {},
   "source": [
    "Why we use **log-return-periods(probability)**:\n",
    "- log-normality\n",
    "- approximate raw-log equality\n",
    "- time-additivity\n",
    "- mathematical ease\n",
    "- numerical stability <br>\n",
    "\n",
    "ref: [ref1](https://quantivity.wordpress.com/2011/02/21/why-log-returns/),\n",
    "[ref2](https://gregorygundersen.com/blog/2022/02/06/log-returns/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ccafb6-1402-4b03-80f8-89cc37112b2f",
   "metadata": {},
   "source": [
    "According to this definition, any variable that is measurable and considered to have a statistical relationship with the dependent variable would qualify as a potential **covariate**. A **covariate** is thus a possible predictive or explanatory variable of the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bbbd98-eaf3-4184-8654-d10b91fdac16",
   "metadata": {},
   "source": [
    "**The binomial distribution** is the discrete probability distribution that gives only two possible results in an experiment, either success or failure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9c3cc4-e9ab-4304-a7dc-b3f9fb1eadda",
   "metadata": {},
   "source": [
    "**A risk ratio (RR)**, also called **relative risk**, compares the risk of a health event (disease, injury, risk factor, or death) among one group with the risk among another group. It does so by dividing the risk (incidence proportion, attack rate) in group 1 by the risk (incidence proportion, attack rate) in group 2\n",
    "\n",
    "For our work: it is for counts of events relative to possible number of events (binomial). **The risk ratio** is the ratio of the probability of an event under the model fit to the first dataset to the probability under the model fit to the second dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f3e56a-62ce-4e56-9134-086581ccb01f",
   "metadata": {},
   "source": [
    "**Maximum likelihood estimation (MLE)** is a method that determines values for the parameters of a model. The parameter values are found such that they maximise the likelihood that the process described by the model produced the data that were actually observed.\n",
    "\n",
    "There is a technique that can help us find maxima (and minima) of functions. It’s called **differentiation**. All we have to do is find the **derivative** of the function, set the derivative function to zero and then rearrange the equation to make the parameter of interest the subject of the equation. And voilà, we’ll have our **MLE** values for our parameters.\n",
    "\n",
    "Ref: [ref](https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6747d92-7b56-4be3-8083-04ae7b5a1538",
   "metadata": {},
   "source": [
    "**MLE** is **asymtoptically** efficient: in the limit, a maximum likelihood estimator achieves minimum possible variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d11dc4-9fb5-4452-a4b6-3cb277e80d88",
   "metadata": {},
   "source": [
    "**Bootstrapping** is a statistical procedure that resamples a single dataset to create many simulated samples. This process allows you to calculate **standard errors**, construct **confidence intervals**, and perform hypothesis testing for numerous types of sample statistics. **Bootstrap** methods are alternative approaches to traditional hypothesis testing and are notable for being easier to understand and valid for more conditions.\n",
    "\n",
    "Ref: [ref](https://statisticsbyjim.com/hypothesis-testing/bootstrapping/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514e8a93-8ada-4b72-b726-63f90dc97182",
   "metadata": {},
   "source": [
    "In statistics, **the likelihood-ratio test** assesses the goodness of fit of two competing statistical models based on the ratio of their likelihoods, specifically one found by maximization over the entire parameter space and another found after imposing some constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff403cfe-3870-416a-aef2-f451a09d5064",
   "metadata": {},
   "source": [
    "**A confidence interval**, in statistics, refers to the probability that a population parameter will fall between a set of values for a certain proportion of times. Analysts often use confidence intervals than contain either 95% or 99% of expected observations. Thus, if a point estimate is generated from a statistical model of 10.00 with a 95% confidence interval of 9.50 - 10.50, it can be inferred that there is a 95% probability that the true value falls within that range.\n",
    "\n",
    "They are also used in **hypothesis testing** and regression analysis.\n",
    "Statisticians often use **p-values** in conjunction with **confidence intervals** to gauge **statistical significance**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a0ac89-b5d0-42ce-bce0-5ece485090fe",
   "metadata": {},
   "source": [
    "Note that if only one endpoint of the resulting interval is used, for example the lower bound, then the effective confidence level increases by half of one minus confidence level. For example, a two-sided 0.90 **confidence interval** corresponds to a one-sided 0.95 confidence interval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e144a3d6-805f-470a-831d-98243237aeee",
   "metadata": {},
   "source": [
    "***NOW READ: [DOCUMENTATION](https://cran.r-project.org/web/packages/climextRemes/climextRemes.pdf)***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937e4aae-c933-424d-8389-07f1e7731675",
   "metadata": {},
   "source": [
    "## 4. Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3b7d22-6c00-4b93-95c3-d625f68f835b",
   "metadata": {},
   "source": [
    "### 3.1 calc_logReturnPeriod_fevd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65ec11ec-7e70-46c3-8287-e7aeddb76ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function calc_logReturnPeriod_fevd:\n",
      "\n",
      "calc_logReturnPeriod_fevd(fit=None, returnValue=None, covariates=None, upper=False)\n",
      "    Calculates log return period and standard error given return value(s) of interest \n",
      "    \n",
      "    **description**\n",
      "    \n",
      "     Calculates log return period given return value(s) of interest, using model fit from  extRemes::fevd . Standard error is obtained via the delta method. The return period is the average number of blocks expected to occur before the return value is exceeded and is equal to the inverse of the probability of exceeding the return value in a single block. For non-stationary models (those that include covariates for the location, scale, and/or shape parameters, log return periods and standard errors are returned for as many sets of covariates as provided.\n",
      "    \n",
      "    **arguments**\n",
      "    \n",
      "     fit: fitted object from extRemes fevd\n",
      "    \n",
      "     returnValue: value(s) for which return period is desired\n",
      "    \n",
      "     covariates: matrix of covariate values, each row a set of covariates for which the log return period is desired\n",
      "    \n",
      "     upper: logical value indicating whether upper tail or lower tail is being considered\n",
      "    \n",
      "    \n",
      "    \n",
      "    **details**\n",
      "    \n",
      "     Results are calculated (and returned) on log scale as delta-method based standard errors are more accurate for the log period. Confidence intervals on the return period scale should be calculated by calculating a confidence interval for the log return period and exponentiating the endpoints of the interval.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(climextremes.calc_logReturnPeriod_fevd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2974eaaf-3b8e-41bf-a781-9c04fa82b78f",
   "metadata": {},
   "source": [
    "### 3.2 calc_logReturnProbDiff_fevd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b461cadb-713b-41d3-8f65-dbca47e48e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function calc_logReturnProbDiff_fevd:\n",
      "\n",
      "calc_logReturnProbDiff_fevd(fit=None, returnValue=None, covariates1=None, covariates2=None, getSE=True, scaling=1.0, upper=False)\n",
      "    Calculates log return probability difference for two sets of covariates and standard error of difference given return value(s) of interest \n",
      "    \n",
      "    **description**\n",
      "    \n",
      "     Calculates difference in log return probabilities for two sets of covariates given return value(s) of interest, using model fit from  extRemes::fevd . Standard error is obtained via the delta method. The return probability is the probability of exceeding the return value in a single block. Differences and standard errors are returned for as many contrasts between covariate sets as provided.\n",
      "    \n",
      "    **arguments**\n",
      "    \n",
      "     fit: fitted object from extRemes fevd\n",
      "    \n",
      "     returnValue: value(s) for which the log return probability difference is desired\n",
      "    \n",
      "     covariates1: matrix of covariate values, each row a set of covariates for which the log return probability difference relative to the corresponding row of covariates2 is desired\n",
      "    \n",
      "     covariates2: matrix of covariate values, each row a set of covariates\n",
      "    \n",
      "     getSE: logical indicating whether standard error is desired, in addition to the point estimate\n",
      "    \n",
      "     scaling: if returnValue is scaled for numerics, this allows names of output to be on original scale\n",
      "    \n",
      "     upper: logical value indicating whether upper tail or lower tail is being considered\n",
      "    \n",
      "    \n",
      "    \n",
      "    **details**\n",
      "    \n",
      "     Results are calculated (and returned) on log scale as delta-method based standard errors are more accurate for the log probability. Confidence intervals for the ratio of return probabilities should be calculated by calculating a confidence interval for the log probability difference and exponentiating the endpoints of the interval.\n",
      "    \n",
      "     This is designed to calculate differences in log return probabilities and associated standard errors for different covariate values based on the same model fit. It is not designed for differences based on separate model fits, although it may be possible handle this case by' fit 'two models in a single model specification using dummy variables.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(climextremes.calc_logReturnProbDiff_fevd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adc8d08-5a3f-40da-9eed-bca048be5b01",
   "metadata": {},
   "source": [
    "### 3.3 calc_logReturnProb_fevd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b42fb3a-9331-4cfd-8d96-57ae1a1ca111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function calc_logReturnProb_fevd:\n",
      "\n",
      "calc_logReturnProb_fevd(fit=None, returnValue=None, covariates=None, getSE=True, scaling=1.0, upper=False)\n",
      "    Calculates log return probability and standard error given return value(s) of interest \n",
      "    \n",
      "    **description**\n",
      "    \n",
      "     Calculates log return probability given return value(s) of interest, using model fit from  extRemes::fevd . Standard error is obtained via the delta method. The return probability is the probability of exceeding the return value in a single block. For non-stationary models (those that include covariates for the location, scale, and/or shape parameters, log probabilities and standard errors are returned for as many sets of covariates as provided.\n",
      "    \n",
      "    **arguments**\n",
      "    \n",
      "     fit: fitted object from extRemes fevd\n",
      "    \n",
      "     returnValue: value(s) for which log return probability is desired\n",
      "    \n",
      "     covariates: matrix of covariate values, each row a set of covariates for which the return probability is desired\n",
      "    \n",
      "     getSE: logical indicating whether standard error is desired, in addition to the point estimate\n",
      "    \n",
      "     scaling: if returnValue is scaled for numerics, this allows names of output to be on original scale\n",
      "    \n",
      "     upper: logical value indicating whether upper tail or lower tail is being considered\n",
      "    \n",
      "    \n",
      "    \n",
      "    **details**\n",
      "    \n",
      "     Results are calculated (and returned) on log scale as delta-method based standard errors are more accurate for the log probability. Confidence intervals on the probability scale should be calculated by calculating a confidence interval for the log probability and exponentiating the endpoints of the interval.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(climextremes.calc_logReturnProb_fevd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a43f761-cc80-432a-ba7a-1fc26de2223d",
   "metadata": {},
   "source": [
    "### 3.4 calc_returnValueDiff_fevd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a7ba04c-d4e0-4da8-b8d3-0465c3faf5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function calc_returnValueDiff_fevd:\n",
      "\n",
      "calc_returnValueDiff_fevd(fit=None, returnPeriod=None, covariates1=None, covariates2=None, getSE=True)\n",
      "    Calculates return value difference for two sets of covariates and standard error of difference given return period(s) of interest \n",
      "    \n",
      "    **description**\n",
      "    \n",
      "     Calculates difference in return values (also known as return levels) for two sets of covariates given return period(s) of interest, using model fit from  extRemes::fevd . Standard error is obtained via the delta method. The return value is the value for which the expected number of blocks until an event that exceeds that value is equal to the return period. Differences and standard errors are returned for as many contrasts between covariate sets as provided.\n",
      "    \n",
      "    **arguments**\n",
      "    \n",
      "     fit: fitted object from extRemes fevd\n",
      "    \n",
      "     returnPeriod: value(s) for which return value difference is desired\n",
      "    \n",
      "     covariates1: matrix of covariate values, each row a set of covariates for which the return value difference relative to the corresponding row of covariates2 is desired\n",
      "    \n",
      "     covariates2: matrix of covariate values, each row a set of covariates\n",
      "    \n",
      "     getSE: logical indicating whether standard error is desired, in addition to the point estimate\n",
      "    \n",
      "    \n",
      "    \n",
      "    **details**\n",
      "    \n",
      "     This is designed to calculate differences in return values and associated standard errors for different covariate values based on the same model fit. It is not designed for differences based on separate model fits, although it may be possible handle this case by' fit 'two models in a single model specification using dummy variables.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(climextremes.calc_returnValueDiff_fevd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7478f2f7-e583-4ffa-a743-b01bb98be6e1",
   "metadata": {},
   "source": [
    "### 3.5 calc_returnValue_fevd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59ec9f1c-740e-428f-991b-b20f21ef0991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function calc_returnValue_fevd:\n",
      "\n",
      "calc_returnValue_fevd(fit=None, returnPeriod=None, covariates=None)\n",
      "    Calculates return value and standard error given return period(s) of interest \n",
      "    \n",
      "    **description**\n",
      "    \n",
      "     Calculates return value (also known as the return level) given return period(s) of interest, using model fit from  extRemes::fevd . Standard error is obtained via the delta method. The return value is the value for which the expected number of blocks until an event that exceeds that value is equal to the return period. For non-stationary models (those that include covariates for the location, scale, and/or shape parameters, return values and standard errors are returned for as many sets of covariates as provided.\n",
      "    \n",
      "    **arguments**\n",
      "    \n",
      "     fit: fitted object from extRemes fevd\n",
      "    \n",
      "     returnPeriod: value(s) for which return value is desired\n",
      "    \n",
      "     covariates: matrix of covariate values, each row a set of covariates for which the return value is desired\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(climextremes.calc_returnValue_fevd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2762441c-fd0d-49f0-93a2-0a54ae30f5d9",
   "metadata": {},
   "source": [
    "### 3.6 calc_riskRatio_binom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecfeb284-527d-458b-8166-8f1f1f20d1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function calc_riskRatio_binom:\n",
      "\n",
      "calc_riskRatio_binom(y=None, n=None, ciLevel=0.9, ciType=None, bootSE=None, bootControl=None, lrtControl=None)\n",
      "    Compute risk ratio and uncertainty based on binomial models for counts of events relative to possible number of events \n",
      "    \n",
      "    **description**\n",
      "    \n",
      "     Compute risk ratio and uncertainty by fitting binomial models to counts of events relative to possible number of events. The risk ratio is the ratio of the probability of an event under the model fit to the first dataset to the probability under the model fit to the second dataset. Default standard errors are based on the usual MLE asymptotics using a delta-method-based approximation, but standard errors based on the nonparametric bootstrap and on a likelihood ratio procedure can also be computed.\n",
      "    \n",
      "    **arguments**\n",
      "    \n",
      "    y: numpy array of two values, the number of events in the two scenarios.\n",
      "    \n",
      "    n: numpy array of two values, the number of samples (possible occurrences of events) in the two scenarios.\n",
      "    \n",
      "    ciLevel: statistical confidence level for confidence intervals; in repeated experimentation, this proportion of confidence intervals should contain the true risk ratio. Note that if only one endpoint of the resulting interval is used, for example the lower bound, then the effective confidence level increases by half of one minus ‘ciLevel’. For example, a two-sided 0.90 confidence interval corresponds to a one-sided 0.95 confidence interval.\n",
      "    \n",
      "    ciType: string or numpy array of strings indicating which type of confidence intervals to compute. See ‘Details'.\n",
      "    \n",
      "    bootSE: boolean indicating whether to use the bootstrap to estimate the standard error of the risk ratio.\n",
      "    \n",
      "    bootControl: dictionary of control parameters for the bootstrapping, used only when at least one bootstrap confidence interval is requested via ‘ciType’. See ‘Details’.\n",
      "    \n",
      "    lrtControl: dictionary containing a single component, a numpy array named ‘bounds’, which sets the range inside which the algorithm searches for the endpoints of the likelihood ratio-based confidence interval. This avoids numerical issues with endpoints converging to zero and infinity. If an endpoint is not found within the interval, it is set to ‘nan’. Used only when ‘'lrt'’ is one of the ‘ciType’ values.\n",
      "    \n",
      "    **details**\n",
      "    \n",
      "    ' ciType ' can include one or more of the following:  'delta' ,  'koopman' ,  'lrt' ,  'boot_norm' ,  'boot_perc' ,  'boot_basic' ,  'boot_stud' ,  'boot_bca' .  'delta'  uses the delta method to compute an asymptotic interval based on the standard error of the log risk ratio.  'koopman'  uses the method described in Koopman (1984), following the implementation discussed in Fageland et al. (2015), including the calculation of Nam (1995).  'lrt'  inverts a likelihood-ratio test. Bootstrap-based options are the normal-based interval using the bootstrap standard error ( 'boot_norm' ), the percentile bootstrap ( 'boot_perc' ), the basic bootstrap ( 'boot_basic' ), the bootstrap-t ( 'boot_stud' ), and the bootstrap BCA method ( 'boot_bca' ). See Paciorek et al. for more details. \n",
      "    \n",
      "     See  fit_pot  for information on the ' bootControl ' argument.\n",
      "    \n",
      "    **value**\n",
      "    \n",
      "    The primary outputs of this function are as follows: the log of the risk ratio and standard error of that log risk ratio (‘logRiskRatio’ and ‘se_logRiskRatio’) as well the risk ratio itself (‘riskRatio’). The standard error is based on the usual MLE asymptotics using a delta-method-based approximation. If requested via ‘ciType’, confidence intervals will be returned, as discussed in ‘Details’.\n",
      "    \n",
      "    **author**\n",
      "    \n",
      "     Christopher J. Paciorek\n",
      "    \n",
      "    **references**\n",
      "    \n",
      "     Paciorek, C.J., D.A. Stone, and M.F. Wehner. 2018. Quantifying uncertainty in the attribution of human influence on severe weather. Weather and Climate Extremes 20:69-80. arXiv preprint <https://arxiv.org/abs/1706.03388>.\n",
      "    \n",
      "     Koopman, P.A.R. 1984. Confidence intervals for the ratio of two binomial proportions. Biometrics 40: 513-517.\n",
      "    \n",
      "     Fagerland, M.W., S. Lydersen, and P. Laake. 2015. Recommended confidence intervals for two independent binomial proportions. Statistical Methods in Medical Research 24: 224-254.\n",
      "    \n",
      "    **examples**\n",
      "    \n",
      "    >>> result = climextremes.calc_riskRatio_binom(numpy.array((40, 8)), \n",
      "    ...                                            numpy.array((400, 400)),\n",
      "    ...                                            ciType = numpy.array(('lrt', 'boot_stud', 'koopman')))\n",
      "    ... result['logRiskRatio']\n",
      "    ... result['se_logRiskRatio']\n",
      "    ... result['riskRatio']\n",
      "    ... result['ci_riskRatio_lrt']\n",
      "    ... result['ci_riskRatio_koopman']\n",
      "    ... result['ci_riskRatio_boot_stud']\n",
      "    ... \n",
      "    ... # Koopman and LRT method can estimate lower confidence interval endpoint,\n",
      "    ... # even if estimated risk ratio is infinity\n",
      "    ... result = climextremes.calc_riskRatio_binom(numpy.array((4, 0)), \n",
      "    ...                                            numpy.array((100, 100)),\n",
      "    ...                                            ciType = numpy.array(('lrt', 'boot_stud', 'koopman')))\n",
      "    ... result['logRiskRatio']\n",
      "    ... result['se_logRiskRatio']\n",
      "    ... result['riskRatio']\n",
      "    ... result['ci_riskRatio_lrt']\n",
      "    ... result['ci_riskRatio_koopman']\n",
      "    ... result['ci_riskRatio_boot_stud']\n",
      "    ... \n",
      "    ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(climextremes.calc_riskRatio_binom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7893381-a342-4b62-a6cd-adc282622885",
   "metadata": {},
   "source": [
    "#### 3.6.1 Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a40e4100-b249-4706-838d-7493f72e7383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# risk ratio for 40/400 compared to 8/400 events\n",
    "result = climextremes.calc_riskRatio_binom(np.array((40, 8)), \n",
    "                                           np.array((400, 400)),\n",
    "                                           ciType = np.array(('lrt',\n",
    "                                                              'boot_stud',\n",
    "                                                              'koopman'))\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e37a85ca-2930-4cb8-926e-5851c3647117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.60943791])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# log risk ratio between first and second events\n",
    "result['logRiskRatio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05b19cf3-b47d-46ce-bc49-c13203b4b3fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.38078866])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# standard error of log risk ratio (point estimate) based on delta method\n",
    "result['se_logRiskRatio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04dc4b72-10d8-4cfd-a1c6-b0a43d4890b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# raw risk ratio between first and second events\n",
    "result['riskRatio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f556c57-f28e-4494-94e1-0e916861540f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.78391806, 9.86604856])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confidence interval of risk ratio based on likelihood ratio test\n",
    "# between events\n",
    "result['ci_riskRatio_lrt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c52f108-ebf6-47e7-b0bd-3e8260ec133b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.70619164, 9.27887211])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confidence interval of risk ratio based on koopman method\n",
    "# between events\n",
    "result['ci_riskRatio_koopman']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b28f94d0-d77d-4a1b-b159-368ee4c01f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.57683507, 10.92551928])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confidence interval of risk ratio based on students t bootstrap\n",
    "# between events\n",
    "result['ci_riskRatio_boot_stud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "794942f6-535c-4ced-8599-69a88b168a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Koopman and LRT method can estimate lower confidence interval endpoint,\n",
    "# even if estimated risk ratio is infinity\n",
    "# risk ratio for 4/100 compared to 0/100 events\n",
    "result = climextremes.calc_riskRatio_binom(np.array((4, 0)), \n",
    "                                           np.array((100, 100)),\n",
    "                                           ciType = np.array(('lrt',\n",
    "                                                              'koopman'))\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b558a1fe-4e63-4a08-b39c-7919fe33a181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.51387752,        inf])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confidence interval of risk ratio based on likelihood ratio test\n",
    "# between events\n",
    "result['ci_riskRatio_lrt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad194343-62dc-4240-b157-24859ea9dc64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.50254114,        inf])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confidence interval of risk ratio based on koopman method\n",
    "# between events\n",
    "result['ci_riskRatio_koopman']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e741ce99-455f-4e21-b02a-631bbde94775",
   "metadata": {},
   "source": [
    "### 3.7 calc_riskRatio_gev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63d5be28-c227-4f16-b197-5e1f690de6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function calc_riskRatio_gev:\n",
      "\n",
      "calc_riskRatio_gev(returnValue=None, y1=None, y2=None, x1=None, x2=None, locationFun1=None, locationFun2=None, scaleFun1=None, scaleFun2=None, shapeFun1=None, shapeFun2=None, nReplicates1=1.0, nReplicates2=1.0, replicateIndex1=None, replicateIndex2=None, weights1=None, weights2=None, xNew1=None, xNew2=None, maxes=True, scaling1=1.0, scaling2=1.0, ciLevel=0.9, ciType=None, bootSE=None, bootControl=None, lrtControl=None, optimArgs=None, optimControl=None, initial1=None, initial2=None, logScale1=None, logScale2=None, getReturnCalcs=False, getParams=False, getFit=False)\n",
      "    Compute risk ratio and uncertainty based on generalized extreme value model fit to block maxima or minima \n",
      "    \n",
      "    **description**\n",
      "    \n",
      "     Compute risk ratio and uncertainty by fitting generalized extreme value model, designed specifically for climate data, to exceedance-only data, using the point process approach. The risk ratio is the ratio of the probability of exceedance of a pre-specified value under the model fit to the first dataset to the probability under the model fit to the second dataset. Default standard errors are based on the usual MLE asymptotics using a delta-method-based approximation, but standard errors based on the nonparametric bootstrap and on a likelihood ratio procedure can also be computed.\n",
      "    \n",
      "    **arguments**\n",
      "    \n",
      "    returnValue: numeric value giving the value for which the risk ratio should be calculated, where the resulting period will be the average number of blocks until the value is exceeded and the probability the probability of exceeding the value in any single block.\n",
      "    \n",
      "    y1: numpy array of observed maxima or minima values for the first dataset. See ‘Details’ for how the values of ‘y1’ should be ordered if there are multiple replicates and the values of ‘x1’ are identical for all replicates. For better optimization performance, it is recommended that the ‘y1’ have magnitude around one (see ‘Details’), for which one can use ‘scaling1’.\n",
      "    \n",
      "    y2: numpy array of observed maxima or minima values for the second dataset. Analogous to ‘y1’.\n",
      "    \n",
      "    x1: numpy array or pandas data frame with columns corresponding to covariate/predictor/feature variables and each row containing the values of the variable for the corresponding observed maximum/minimum. The number of rows should either equal the length of ‘y1’ or (if there is more than one replicate) it can optionally equal the number of observations in a single replicate, in which case the values will be assumed to be the same for all replicates. \n",
      "    \n",
      "    x2: analogous to ‘x1’ but for the second dataset\n",
      "    \n",
      "    locationFun1: numpy array of either character strings or indices describing a linear model (i.e., regression function) for the location parameter using columns from ‘x1’ for the first dataset. ‘x1’ must be supplied if this is anything other than ‘None’.\n",
      "    \n",
      "    locationFun2: numpy array of either character strings or indices describing a linear model (i.e., regression function) for the location parameter using columns from ‘x2’ for the second dataset. ‘x2’ must be supplied if this is anything other than ‘None’.\n",
      "    \n",
      "    scaleFun1: numpy array of either character strings or indices describing a linear model (i.e., regression function) for the log of the scale parameter using columns from ‘x1’ for the first dataset.  ‘x1’ must be supplied if this is anything other than ‘None’.\n",
      "    \n",
      "    scaleFun2: numpy array of either character strings or indices describing a linear model (i.e., regression function) for the log of the scale parameter using columns from ‘x2’ for the second dataset.  ‘x2’ must be supplied if this is anything other than ‘None’.\n",
      "    \n",
      "    shapeFun1: numpy array of either character strings or indices describing a linear model (i.e., regression function) for the shape parameter using columns from ‘x1’ for the first dataset.  ‘x1’ must be supplied if this is anything other than ‘None’.\n",
      "    \n",
      "    shapeFun2: numpy array of either character strings or indices describing a linear model (i.e., regression function) for the shape parameter using columns from ‘x2’ for the first dataset.  ‘x2’ must be supplied if this is anything other than ‘None’.\n",
      "    \n",
      "    nReplicates1: numeric value indicating the number of replicates for the first dataset.\n",
      "    \n",
      "    nReplicates2: numeric value indicating the number of replicates for the second dataset.\n",
      "    \n",
      "    replicateIndex1: numpy array providing the index of the replicate corresponding to each element of ‘y1’. Used (and therefore required) only when using bootstrapping with the resampling by replicates based on the ‘by’ element of ‘bootControl’.\n",
      "    \n",
      "    replicateIndex2: numpy array providing the index of the replicate corresponding to each element of ‘y2’. Analogous to ‘replicateIndex1’.\n",
      "    \n",
      "    weights1: numpy array providing the weights for each observation in the first dataset. When there is only one replicate or the weights do not vary by replicate, an array of length equal to the number of observations. When weights vary by replicate, this should be of equal length to ‘y’. Likelihood contribution of each observation is multiplied by the corresponding weight. \n",
      "    \n",
      "    weights2: numpy array providing the weights for each observation in the second dataset. Analogous to ‘weights1’.\n",
      "    \n",
      "    xNew1: object of the same form as ‘x1’, providing covariate/predictor/feature values for which one desires log risk ratios.\n",
      "    \n",
      "    xNew2: object of the same form as ‘x2’, providing covariate/predictor/feature values for which log risk ratios are desired. Must provide the same number of covariate sets as ‘xNew1’ as the risk ratio is based on contrasting return probabilities under ‘xNew1’ and ‘xNew2’.\n",
      "    \n",
      "    maxes: boolean indicating whether analysis is for block maxima (‘True’) or block minima (‘False’); in the latter case, the function works with the negative of the values, changing the sign of the resulting location parameters\n",
      "    \n",
      "    scaling1: positive-valued scalar used to scale the data values of the first dataset for more robust optimization performance. When multiplied by the values, it should produce values with magnitude around 1.\n",
      "    \n",
      "    scaling2: positive-valued scalar used to scale the data values of the second dataset for more robust optimization performance. When multiplied by the values, it should produce values with magnitude around 1.\n",
      "    \n",
      "    ciLevel: statistical confidence level for confidence intervals; in repeated experimentation, this proportion of confidence intervals should contain the true risk ratio. Note that if only one endpoint of the resulting interval is used, for example the lower bound, then the effective confidence level increases by half of one minus ‘ciLevel’. For example, a two-sided 0.90 confidence interval corresponds to a one-sided 0.95 confidence interval.\n",
      "    \n",
      "    ciType: string or numpy array of strings indicating which type of confidence intervals to compute. See ‘Details’.\n",
      "    \n",
      "    bootSE: boolean indicating whether to use the bootstrap to estimate the standard error of the risk ratio.\n",
      "    \n",
      "    bootControl: dictionary of control parameters for the bootstrapping, used only when at least one bootstrap confidence interval is requested via ‘ciType’. See ‘Details’.\n",
      "    \n",
      "    lrtControl: dictionary containing a single component, a numpy array named ‘bounds’, which sets the range inside which the algorithm searches for the endpoints of the likelihood ratio-based confidence interval. This avoids numerical issues with endpoints converging to zero and infinity. If an endpoint is not found within the interval, it is set to ‘nan’. Used only when ‘'lrt'’ is one of the ‘ciType’ values.\n",
      "    \n",
      "    optimArgs: a dictionary with named components matching exactly any arguments that the user wishes to pass to R's ‘optim’ function. See ‘help(optim)’ in R for details. Of particular note, ‘'method'’ can be used to choose the optimization method used for maximizing the log-likelihood to fit the model (e.g., ‘'method'’ could be 'BFGS' instead of the default 'Nelder-Mead'). To specify the ‘control’ argument, use ‘optimControl’ rather than including ‘control’ here.\n",
      "    \n",
      "    optimControl: a dictionary with named components matching exactly any elements that the user wishes to pass as the ‘control’ argument to R's ‘optim’ function. For example, ‘control={'maxit': VALUE)’ for a user-specified VALUE can be used to increase the number of iterations if the optimization is converging slowly.\n",
      "    \n",
      "    initial1: a dictionary with components named ‘'location'’, ‘'scale'’, and ‘'shape'’ providing initial parameter values, intended for use in speeding up or enabling optimization when the default initial values are resulting in failure of the optimization; note that use of ‘scaling’, ‘logScale’ and ‘.normalizeX = True’ cause numerical changes in some of the parameters. For example with ‘logScale1 = True’, initial value(s) for ‘'scale'’ should be specified on the log scale.\n",
      "    \n",
      "    initial2: a dictionary of initial parameter values for the second dataset, analogous to ‘initial1’.\n",
      "    \n",
      "    logScale1: boolean indicating whether optimization for the scale parameter should be done on the log scale for the first dataset. By default this is ‘False’ when the scale is not a function of covariates and ‘True’ when the scale is a function of covariates (to ensure the scale is positive regardless of the regression coefficients). \n",
      "    \n",
      "    logScale2: boolean indicating whether optimization for the scale parameter should be done on the log scale for the second dataset. By default this is ‘False’ when the scale is not a function of covariates and ‘True’ when the scale is a function of covariates (to ensure the scale is positive regardless of the regression coefficients). \n",
      "    \n",
      "    getReturnCalcs: boolean indicating whether to return the estimated return values/probabilities/periods from the fitted models. \n",
      "    \n",
      "    getParams: boolean indicating whether to return the fitted parameter values and their standard errors for the fitted models; WARNING: parameter values for models with covariates for the scale parameter must interpreted based on the value of ‘logScale’.\n",
      "    \n",
      "    getFit: boolean indicating whether to return the full fitted models (potentially useful for model evaluation and for understanding optimization problems); note that estimated parameters in the fit object for nonstationary models will not generally match the MLE provided when ‘getParams’ is ‘True’ because covariates are normalized before fitting and the fit object is based on the normalized covariates. Similarly, parameters will not match if ‘scaling’ is not 1.\n",
      "    \n",
      "    **details**\n",
      "    \n",
      "     See  fit_gev  for more details on fitting the block maxima model for each dataset, including details on blocking and replication. Also see  fit_gev  for information on the ' bootControl ' argument.\n",
      "    \n",
      "     Optimization failures:\n",
      "    \n",
      "     It is not uncommon for maximization of the log-likelihood to fail for extreme value models. Please see the help information for  fit_gev . Also note that if the probability in the denominator of the risk ratio is near one, one may achieve better numerical performance by swapping the two datasets and computing the risk ratio for the probability under dataset 2 relative to the probability under dataset 1.\n",
      "    \n",
      "    ' ciType ' can include one or more of the following:  'delta' ,  'lrt' ,  'boot_norm' ,  'boot_perc' ,  'boot_basic' ,  'boot_stud' ,  'boot_bca' .  'delta'  uses the delta method to compute an asymptotic interval based on the standard error of the log risk ratio.  'lrt'  inverts a likelihood-ratio test. Bootstrap-based options are the normal-based interval using the bootstrap standard error ( 'boot_norm' ), the percentile bootstrap ( 'boot_perc' ), the basic bootstrap ( 'boot_basic' ), the bootstrap-t ( 'boot_stud' ), and the bootstrap BCA method ( 'boot_bca' ). See Paciorek et al. for more details. \n",
      "    \n",
      "     See  fit_pot  for information on the ' bootControl ' argument.\n",
      "    \n",
      "    **value**\n",
      "    \n",
      "    The primary outputs of this function are as follows: the log of the risk ratio and standard error of that log risk ratio (‘logRiskRatio’ and ‘se_logRiskRatio’) as well the risk ratio itself (‘riskRatio’). The standard error is based on the usual MLE asymptotics using a delta-method-based approximation. If requested via ‘ciType’, confidence intervals will be returned, as discussed in ‘Details’.\n",
      "    \n",
      "    **author**\n",
      "    \n",
      "     Christopher J. Paciorek\n",
      "    \n",
      "    **references**\n",
      "    \n",
      "     Paciorek, C.J., D.A. Stone, and M.F. Wehner. 2018. Quantifying uncertainty in the attribution of human influence on severe weather. Weather and Climate Extremes 20:69-80. arXiv preprint <https://arxiv.org/abs/1706.03388>.\n",
      "    \n",
      "     Jeon S., C.J. Paciorek, and M.F. Wehner. 2016. Quantile-based bias correction and uncertainty quantification of extreme event attribution statements. Weather and Climate Extremes 12: 24-32. <DOI:10.1016/j.wace.2016.02.001>. arXiv preprint: <http://arxiv.org/abs/1602.04139>.\n",
      "    \n",
      "    **examples**\n",
      "    \n",
      "    >>> Fort = climextremes.Fort\n",
      "    ... earlyPeriod = numpy.array((1900, 1930))\n",
      "    ... earlyYears = numpy.array(range(earlyPeriod[0], earlyPeriod[1]))\n",
      "    ... latePeriod = numpy.array((1970, 2000))\n",
      "    ... lateYears = numpy.array(range(latePeriod[0], latePeriod[1]))\n",
      "    ... \n",
      "    ... FortMax = Fort.groupby('year').max()[['Prec']]\n",
      "    ... FortMax.reset_index(inplace=True)\n",
      "    ... \n",
      "    ... y1 = FortMax.Prec[FortMax.year < earlyPeriod[1]]\n",
      "    ... y2 = FortMax.Prec[FortMax.year >= latePeriod[0]]\n",
      "    ... \n",
      "    ... # contrast late period with early period, assuming a nonstationary fit\n",
      "    ... # within each time period and finding RR based on midpoint of each period\n",
      "    ... result = climextremes.calc_riskRatio_gev(\n",
      "    ...     returnValue = 3,\n",
      "    ...     y1 = numpy.array(y1), y2 = numpy.array(y2),\n",
      "    ...     x1 = earlyYears, x2 = lateYears,\n",
      "    ...     locationFun1 = 1, locationFun2 = 1,\n",
      "    ...     xNew1 = earlyYears.mean(), xNew2 = lateYears.mean(),\n",
      "    ...     ciType = 'lrt')\n",
      "    ... \n",
      "    ... result['logRiskRatio']\n",
      "    ... result['se_logRiskRatio']\n",
      "    ... result['riskRatio']\n",
      "    ... result['ci_riskRatio_lrt']\n",
      "    ... \n",
      "    ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(climextremes.calc_riskRatio_gev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3505355d-e835-4a38-99f2-d48578942787",
   "metadata": {},
   "source": [
    "#### 3.7.1 Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1137352-3b05-4295-b3a2-58d97489f749",
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyPeriod = np.array((1900, 1930))\n",
    "earlyYears = np.array(range(earlyPeriod[0], earlyPeriod[1]))\n",
    "\n",
    "latePeriod = np.array((1970, 2000))\n",
    "lateYears = np.array(range(latePeriod[0], latePeriod[1]))\n",
    "\n",
    "dtMax = dt.groupby('year').max()[['Prec']]\n",
    "dtMax.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fdc5b4d0-5e85-4c68-bb0a-c8a3e2ed1c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obs</th>\n",
       "      <th>tobs</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>year</th>\n",
       "      <th>Prec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   obs  tobs  month  day    year  Prec\n",
       "1  1.0   1.0    1.0  1.0  1900.0   0.0\n",
       "2  2.0   2.0    1.0  2.0  1900.0   0.0\n",
       "3  3.0   3.0    1.0  3.0  1900.0   0.0\n",
       "4  4.0   4.0    1.0  4.0  1900.0   0.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f14e3d4-2b74-48dd-82e7-e919151d7cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>Prec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1900.0</td>\n",
       "      <td>2.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1901.0</td>\n",
       "      <td>2.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1902.0</td>\n",
       "      <td>4.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1903.0</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     year  Prec\n",
       "0  1900.0  2.39\n",
       "1  1901.0  2.32\n",
       "2  1902.0  4.34\n",
       "3  1903.0  0.85"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtMax.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0a4b333-b808-4c43-aaf4-ac6d13bee301",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = dtMax.Prec[dtMax.year < earlyPeriod[1]]\n",
    "y2 = dtMax.Prec[dtMax.year >= latePeriod[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb2199d2-2c64-4009-9731-c05144607109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     2.39\n",
       "1     2.32\n",
       "2     4.34\n",
       "3     0.85\n",
       "4     3.02\n",
       "5     1.74\n",
       "6     1.70\n",
       "7     1.21\n",
       "8     1.93\n",
       "9     1.68\n",
       "10    1.48\n",
       "11    1.16\n",
       "12    1.32\n",
       "13    1.32\n",
       "14    1.44\n",
       "15    1.83\n",
       "16    2.23\n",
       "17    0.96\n",
       "18    2.98\n",
       "19    0.97\n",
       "20    1.16\n",
       "21    1.46\n",
       "22    0.84\n",
       "23    2.30\n",
       "24    1.38\n",
       "25    1.70\n",
       "26    1.17\n",
       "27    1.15\n",
       "28    1.32\n",
       "29    1.25\n",
       "Name: Prec, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e46abf6-8798-48c7-b397-665a921941fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contrast late period with early period, assuming a nonstationary fit\n",
    "# within each time period and finding RR based on midpoint of each period\n",
    "\n",
    "result = climextremes.calc_riskRatio_gev(\n",
    "    returnValue = 3,\n",
    "    y1 = np.array(y1), y2 = np.array(y2),\n",
    "    x1 = earlyYears, x2 = lateYears,\n",
    "    locationFun1 = 1, locationFun2 = 1,\n",
    "    xNew1 = int(earlyYears.mean()), xNew2 = int(lateYears.mean()),\n",
    "    ciType = 'lrt'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4cd1c310-9d14-4bf0-883b-d95faa9170d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logRiskRatio': array([-0.74591629]),\n",
       " 'se_logRiskRatio': array([0.76214254]),\n",
       " 'riskRatio': array([0.4742995]),\n",
       " 'ci_riskRatio_lrt_names': array(['0.05', '0.95'], dtype='<U4'),\n",
       " 'ci_riskRatio_lrt': array([0.13618289, 1.5597101 ])}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa476d1a-a89d-4970-9463-372cb2c7043f",
   "metadata": {},
   "source": [
    "### 3.8 calc_riskRatio_pot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e79c9e39-79db-423a-ab71-a606c57f8c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function calc_riskRatio_pot:\n",
      "\n",
      "calc_riskRatio_pot(returnValue=None, y1=None, y2=None, x1=None, x2=None, threshold1=None, threshold2=None, locationFun1=None, locationFun2=None, scaleFun1=None, scaleFun2=None, shapeFun1=None, shapeFun2=None, nBlocks1=None, nBlocks2=None, blockIndex1=None, blockIndex2=None, firstBlock1=1.0, firstBlock2=1.0, index1=None, index2=None, nReplicates1=1.0, nReplicates2=1.0, replicateIndex1=None, replicateIndex2=None, weights1=None, weights2=None, proportionMissing1=None, proportionMissing2=None, xNew1=None, xNew2=None, declustering=None, upperTail=True, scaling1=1.0, scaling2=1.0, ciLevel=0.9, ciType=None, bootSE=None, bootControl=None, lrtControl=None, optimArgs=None, optimControl=None, initial1=None, initial2=None, logScale1=None, logScale2=None, getReturnCalcs=False, getParams=False, getFit=False)\n",
      "    Compute risk ratio and uncertainty based on peaks-over-threshold models fit to exceedances over a threshold \n",
      "    \n",
      "    **description**\n",
      "    \n",
      "     Compute risk ratio and uncertainty by fitting peaks-over-threshold model, designed specifically for climate data, to exceedance-only data, using the point process approach. The risk ratio is the ratio of the probability of exceedance of a pre-specified value under the model fit to the first dataset to the probability under the model fit to the second dataset. Default standard errors are based on the usual MLE asymptotics using a delta-method-based approximation, but standard errors based on the nonparametric bootstrap and on a likelihood ratio procedure can also be computed.\n",
      "    \n",
      "    **arguments**\n",
      "    \n",
      "    returnValue: numeric value giving the value for which the risk ratio should be calculated, where the resulting period will be the average number of blocks until the value is exceeded and the probability the probability of exceeding the value in any single block.\n",
      "    \n",
      "    y1: numpy array of exceedance values for the first dataset (values of the outcome variable above the threshold). For better optimization performance, it is recommended that the ‘y’ have magnitude around one (see ‘Details’), for which one can use ‘scaling1’.\n",
      "    \n",
      "    y2: numpy array of exceedance values for the second dataset (values of the outcome variable above the threshold).\n",
      "    \n",
      "    x1: numpy array or pandas data frame with columns corresponding to covariate/predictor/feature variables and each row containing the values of the variable for a block (e.g., often a year with climate data) for the first dataset. The number of rows must equal the number of blocks.\n",
      "    \n",
      "    x2: analogous to ‘x1’ but for the second dataset\n",
      "    \n",
      "    threshold1: a single numeric value for constant threshold or a numpy array with length equal to the number of blocks, indicating the threshold for each block for the first dataset.\n",
      "    \n",
      "    threshold2: analogous to ‘threshold1’ but for the second dataset\n",
      "    \n",
      "    locationFun1: numpy array of either character strings or indices describing a linear model (i.e., regression function) for the location parameter using columns from ‘x1’ for the first dataset. ‘x1’ must be supplied if this is anything other than ‘None’.\n",
      "    \n",
      "    locationFun2: numpy array of either character strings or indices describing a linear model (i.e., regression function) for the location parameter using columns from ‘x2’ for the second dataset. ‘x2’ must be supplied if this is anything other than ‘None’.\n",
      "    \n",
      "    scaleFun1: numpy array of either character strings or indices describing a linear model (i.e., regression function) for the log of the scale parameter using columns from ‘x1’ for the first dataset.  ‘x1’ must be supplied if this is anything other than ‘None’.\n",
      "    \n",
      "    scaleFun2: numpy array of either character strings or indices describing a linear model (i.e., regression function) for the log of the scale parameter using columns from ‘x2’ for the second dataset.  ‘x2’ must be supplied if this is anything other than ‘None’.\n",
      "    \n",
      "    shapeFun1: numpy array of either character strings or indices describing a linear model (i.e., regression function) for the shape parameter using columns from ‘x1’ for the first dataset.  ‘x1’ must be supplied if this is anything other than ‘None’.\n",
      "    \n",
      "    shapeFun2: numpy array of either character strings or indices describing a linear model (i.e., regression function) for the shape parameter using columns from ‘x2’ for the first dataset.  ‘x2’ must be supplied if this is anything other than ‘None’.\n",
      "    \n",
      "    nBlocks1: number of blocks (e.g., a block will often be a year with climate data) in first dataset; note this value determines the interpretation of return values/periods/probabilities; see ‘returnPeriod’ and ‘returnValue’.\n",
      "    \n",
      "    nBlocks2: number of blocks (e.g., a block will often be a year with climate data) in second dataset; note this value determines the interpretation of return values/periods/probabilities; see ‘returnPeriod’ and ‘returnValue’.\n",
      "    \n",
      "    blockIndex1: numpy array providing the index of the block corresponding to each element of ‘y1’. Used only when ‘x1’ is provided to match exceedances to the covariate/predictor/feature value for the exceedance or when using bootstrapping with the resampling based on blocks based on the ‘by’ element of ‘bootControl’. If ‘firstBlock1’ is not equal to one, then ‘blockIndex1’ need not have one as its smallest possible value.\n",
      "    \n",
      "    blockIndex2: numpy array providing the index of the block corresponding to each element of ‘y2’. Analogous to ‘blockIndex1’.\n",
      "    \n",
      "    firstBlock1: single numeric value indicating the numeric value of the first possible block of ‘blockIndex1’. For example the values in ‘blockIndex1’ might indicate the year of each exceedance with the first year of data being 1969, in which case ‘firstBlock1’ would be 1969. Note that the first block may not have any exceedances so it may not be represented in ‘blockIndex1’. Used only to adjust ‘blockIndex1’ so that the block indices start at one and therefore correspond to the rows of ‘x1’.\n",
      "    \n",
      "    firstBlock2: single numeric value indicating the numeric value of the first possible block of ‘blockIndex2’. Analogous to ‘firstBlock1’.\n",
      "    \n",
      "    index1: (optional) numpy array providing the integer-valued index (e.g., julian day for daily climate data) corresponding to each element of ‘y1’. For example if there are 10 original observations and the third, fourth, and seventh values are exceedances, then ‘index1’ would be the vector 3,4,7. Used only when ‘declustering’ is provided to determine which exceedances occur sequentially or within a contiguous set of values of a given length. The actual values are arbitrary; only the lags between the values are used.\n",
      "    \n",
      "    index2: (optional) numpy array providing the integer-valued index (e.g., julian day for daily climate data) corresponding to each element of ‘y2’. Analogous to ‘index1’.\n",
      "    \n",
      "    nReplicates1: numeric value indicating the number of replicates for the first dataset.\n",
      "    \n",
      "    nReplicates2: numeric value indicating the number of replicates for the second dataset.\n",
      "    \n",
      "    replicateIndex1: numpy array providing the index of the replicate corresponding to each element of ‘y1’. Used for three purposes: (1) when using bootstrapping with the resampling based on replicates based on the ‘by’ element of ‘bootControl’, (2) to avoid treating values in different replicates as potentially being sequential or within a short interval when removing values based on ‘declustering’, and (3) to match outcomes to ‘weights’ or ‘proportionMissing’ when either vary by replicate.  \n",
      "    \n",
      "    replicateIndex2: numpy array providing the index of the replicate corresponding to each element of ‘y2’. Analogous to ‘replicateIndex1’.\n",
      "    \n",
      "    weights1: numpy array providing the weights by block for the first dataset. When there is only one replicate or the weights do not vary by replicate, a one-dimensional array of length equal to the number of blocks. When weights vary by replicate, a two-dimensional array with rows corresponding to blocks and columns to replicates. Likelihood contribution of each block is multiplied by the corresponding weight. \n",
      "    \n",
      "    weights2: numpy array providing the weights by block for the second dataset. Analogous to ‘weights1’.\n",
      "    \n",
      "    proportionMissing1: a numeric value or numpy array indicating the proportion of missing values in the original first dataset before exceedances were selected. When the proportion missing is the same for all blocks and replicates, a single value. When there is only one replicate or the weights do not vary by replicate, a one-dimensional array of length equal to the number of blocks. When weights vary by replicate, a two-dimensional array with rows corresponding to blocks and columns to replicates.\n",
      "    \n",
      "    proportionMissing2: a numeric value or numpy array indicating the proportion of missing values in the original second dataset before exceedances were selected. Analogous to ‘proportionMissing1’.\n",
      "    \n",
      "    xNew1: object of the same form as ‘x1’, providing covariate/predictor/feature values for which log risk ratios are desired.\n",
      "    \n",
      "    xNew2: object of the same form as ‘x2’, providing covariate/predictor/feature values for which log risk ratios are desired. Must provide the same number of covariate sets as ‘xNew1’ as the risk ratio is based on contrasting return probabilities under ‘xNew1’ and ‘xNew2’.\n",
      "    \n",
      "    declustering: one of ‘None’, ‘'noruns'’, or a number. If ‘'noruns'’ is specified, only the maximum (or minimum if ‘upperTail = False’) value within a set of exceedances corresponding to successive indices is included. If a number, this should indicate the size of the interval (which will be used with the ‘index’ argument) within which to allow only the largest (or smallest if ‘upperTail = False’) value.\n",
      "    \n",
      "    upperTail: boolean indicating whether one is working with exceedances over a high threshold (‘True’) or exceedances under a low threshold (‘False’); in the latter case, the function works with the negative of the values and the threshold, changing the sign of the resulting location parameters.\n",
      "    \n",
      "    scaling1: positive-valued scalar used to scale the data values of the first dataset for more robust optimization performance. When multiplied by the values, it should produce values with magnitude around 1.\n",
      "    \n",
      "    scaling2: positive-valued scalar used to scale the data values of the second dataset for more robust optimization performance. When multiplied by the values, it should produce values with magnitude around 1.\n",
      "    \n",
      "    ciLevel: statistical confidence level for confidence intervals; in repeated experimentation, this proportion of confidence intervals should contain the true risk ratio. Note that if only one endpoint of the resulting interval is used, for example the lower bound, then the effective confidence level increases by half of one minus ‘ciLevel’. For example, a two-sided 0.90 confidence interval corresponds to a one-sided 0.95 confidence interval.\n",
      "    \n",
      "    ciType: numpy string or numpy array of strings indicating which type of confidence intervals to compute. See ‘Details’.\n",
      "    \n",
      "    bootSE: boolean indicating whether to use the bootstrap to estimate the standard error of the risk ratio.\n",
      "    \n",
      "    bootControl: dictionary of control parameters for the bootstrapping, used only when at least one bootstrap confidence interval is requested via ‘ciType’. See ‘Details’.\n",
      "    \n",
      "    lrtControl: dictionary containing a single component, a numpy array named ‘bounds’, which sets the range inside which the algorithm searches for the endpoints of the likelihood ratio-based confidence interval. This avoids numerical issues with endpoints converging to zero and infinity. If an endpoint is not found within the interval, it is set to ‘nan’. Used only when ‘'lrt'’ is one of the ‘ciType’ values.\n",
      "    \n",
      "    optimArgs: a dictionary with named components matching exactly any arguments that the user wishes to pass to R's ‘optim’ function. See ‘help(optim)’ in R for details. Of particular note, ‘'method'’ can be used to choose the optimization method used for maximizing the log-likelihood to fit the model (e.g., ‘'method'’ could be 'BFGS' instead of the default 'Nelder-Mead'). To specify the ‘control’ argument, use ‘optimControl’ rather than including ‘control’ here.\n",
      "    \n",
      "    optimControl: a dictionary with named components matching exactly any elements that the user wishes to pass as the ‘control’ argument to R's ‘optim’ function. For example, ‘control={'maxit': VALUE)’ for a user-specified VALUE can be used to increase the number of iterations if the optimization is converging slowly.\n",
      "    \n",
      "    initial1: a dictionary with components named ‘'location'’, ‘'scale'’, and ‘'shape'’ providing initial parameter values, intended for use in speeding up or enabling optimization when the default initial values are resulting in failure of the optimization; note that use of ‘scaling’, ‘logScale’ and ‘.normalizeX = True’ cause numerical changes in some of the parameters. For example with ‘logScale1 = True’, initial value(s) for ‘'scale'’ should be specified on the log scale.\n",
      "    \n",
      "    initial2: a dictionary of initial parameter values for the second dataset, analogous to ‘initial1’.\n",
      "    \n",
      "    logScale1: boolean indicating whether optimization for the scale parameter should be done on the log scale for the first dataset. By default this is ‘False’ when the scale is not a function of covariates and ‘True’ when the scale is a function of covariates (to ensure the scale is positive regardless of the regression coefficients). \n",
      "    \n",
      "    logScale2: boolean indicating whether optimization for the scale parameter should be done on the log scale for the second dataset. By default this is ‘False’ when the scale is not a function of covariates and ‘True’ when the scale is a function of covariates (to ensure the scale is positive regardless of the regression coefficients). \n",
      "    \n",
      "    getReturnCalcs: boolean indicating whether to return the estimated return values/probabilities/periods from the fitted models. \n",
      "    \n",
      "    getParams: boolean indicating whether to return the fitted parameter values and their standard errors for the fitted models; WARNING: parameter values for models with covariates for the scale parameter must interpreted based on the value of ‘logScale’.\n",
      "    \n",
      "    getFit: boolean indicating whether to return the full fitted models (potentially useful for model evaluation and for understanding optimization problems); note that estimated parameters in the fit object for nonstationary models will not generally match the MLE provided when ‘getParams’ is ‘True’ because covariates are normalized before fitting and the fit object is based on the normalized covariates. Similarly, parameters will not match if ‘scaling’ is not 1.\n",
      "    \n",
      "    **details**\n",
      "    \n",
      "     See  fit_pot  for more details on fitting the peaks-over-threshold model for each dataset, including details on blocking and replication. Also see  fit_pot  for information on the ' bootControl ' argument. \n",
      "    \n",
      "     Optimization failures:\n",
      "    \n",
      "     It is not uncommon for maximization of the log-likelihood to fail for extreme value models. Please see the help information for  fit_pot . Also note that if the probability in the denominator of the risk ratio is near one, one may achieve better numerical performance by swapping the two datasets and computing the risk ratio for the probability under dataset 2 relative to the probability under dataset 1.\n",
      "    \n",
      "    ' ciType ' can include one or more of the following:  'delta' ,  'lrt' ,  'boot_norm' ,  'boot_perc' ,  'boot_basic' ,  'boot_stud' ,  'boot_bca' .  'delta'  uses the delta method to compute an asymptotic interval based on the standard error of the log risk ratio.  'lrt'  inverts a likelihood-ratio test. Bootstrap-based options are the normal-based interval using the bootstrap standard error ( 'boot_norm' ), the percentile bootstrap ( 'boot_perc' ), the basic bootstrap ( 'boot_basic' ), the bootstrap-t ( 'boot_stud' ), and the bootstrap BCA method ( 'boot_bca' ). See Paciorek et al. for more details. \n",
      "    \n",
      "     See  fit_pot  for information on the ' bootControl ' argument.\n",
      "    \n",
      "    **value**\n",
      "    \n",
      "    The primary outputs of this function are as follows: the log of the risk ratio and standard error of that log risk ratio (‘logRiskRatio’ and ‘se_logRiskRatio’) as well the risk ratio itself (‘riskRatio’). The standard error is based on the usual MLE asymptotics using a delta-method-based approximation. If requested via ‘ciType’, confidence intervals will be returned, as discussed in ‘Details’.\n",
      "    \n",
      "    **author**\n",
      "    \n",
      "     Christopher J. Paciorek\n",
      "    \n",
      "    **references**\n",
      "    \n",
      "     Paciorek, C.J., D.A. Stone, and M.F. Wehner. 2018. Quantifying uncertainty in the attribution of human influence on severe weather. Weather and Climate Extremes 20:69-80. arXiv preprint <https://arxiv.org/abs/1706.03388>.\n",
      "    \n",
      "     Jeon S., C.J. Paciorek, and M.F. Wehner. 2016. Quantile-based bias correction and uncertainty quantification of extreme event attribution statements. Weather and Climate Extremes 12: 24-32. <DOI:10.1016/j.wace.2016.02.001>. arXiv preprint: <http://arxiv.org/abs/1602.04139>.\n",
      "    \n",
      "    **examples**\n",
      "    \n",
      "    >>> Fort = climextremes.Fort\n",
      "    ... threshold = 0.395\n",
      "    ... FortExc = Fort[Fort.Prec > threshold]\n",
      "    ... \n",
      "    ... earlyPeriod = numpy.array((1900, 1930))\n",
      "    ... earlyYears = numpy.array(range(earlyPeriod[0], earlyPeriod[1]))\n",
      "    ... latePeriod = numpy.array((1970, 2000))\n",
      "    ... lateYears = numpy.array(range(latePeriod[0], latePeriod[1]))\n",
      "    ... \n",
      "    ... y1 = FortExc.Prec[FortExc.year < earlyPeriod[1]]\n",
      "    ... y2 = FortExc.Prec[FortExc.year >= latePeriod[0]]\n",
      "    ... block1 = FortExc.year[FortExc.year < earlyPeriod[1]]\n",
      "    ... block2 = FortExc.year[FortExc.year >= latePeriod[0]]\n",
      "    ... \n",
      "    ... # contrast late period with early period, assuming a nonstationary fit\n",
      "    ... # within each time period and finding RR based on midpoint of each period\n",
      "    ... result = climextremes.calc_riskRatio_pot(\n",
      "    ...     returnValue = 3,\n",
      "    ...     y1 = numpy.array(y1), y2 = numpy.array(y2),\n",
      "    ...     x1 = earlyYears, x2 = lateYears,\n",
      "    ...     threshold1 = threshold, threshold2 = threshold,\n",
      "    ...     locationFun1 = 1, locationFun2 = 1,\n",
      "    ...     xNew1 = earlyYears.mean(), xNew2 = lateYears.mean(),\n",
      "    ...     blockIndex1 = numpy.array(block1), blockIndex2 = numpy.array(block2),\n",
      "    ...     firstBlock1 = earlyYears[0], firstBlock2 = lateYears[0],\n",
      "    ...     ciType = 'lrt')\n",
      "    ... \n",
      "    ... result['logRiskRatio']\n",
      "    ... result['se_logRiskRatio']\n",
      "    ... result['riskRatio']\n",
      "    ... result['ci_riskRatio_lrt']\n",
      "    ... \n",
      "    ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(climextremes.calc_riskRatio_pot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa66b401-78fd-4f3f-8bde-6fb2d22a7387",
   "metadata": {},
   "source": [
    "#### 3.8.1 Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3189475f-6766-4005-bfba-f8bfe49ec601",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.395\n",
    "dtExc = dt[dt.Prec > threshold]\n",
    "\n",
    "earlyPeriod = np.array((1900, 1930))\n",
    "earlyYears = np.array(range(earlyPeriod[0], earlyPeriod[1]))\n",
    "\n",
    "latePeriod = np.array((1970, 2000))\n",
    "lateYears = np.array(range(latePeriod[0], latePeriod[1]))\n",
    "    \n",
    "y1 = dtExc.Prec[dtExc.year < earlyPeriod[1]]\n",
    "y2 = dtExc.Prec[dtExc.year >= latePeriod[0]]\n",
    "\n",
    "block1 = dtExc.year[dtExc.year < earlyPeriod[1]]\n",
    "block2 = dtExc.year[dtExc.year >= latePeriod[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "118ad730-298d-43b7-9520-6699677222d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obs</th>\n",
       "      <th>tobs</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>year</th>\n",
       "      <th>Prec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>86.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>94.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>1.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     obs  tobs  month   day    year  Prec\n",
       "86  86.0  86.0    3.0  27.0  1900.0  0.57\n",
       "94  94.0  94.0    4.0   4.0  1900.0  1.52\n",
       "95  95.0  95.0    4.0   5.0  1900.0  0.49\n",
       "99  99.0  99.0    4.0   9.0  1900.0  0.91"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtExc.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bcb6495c-5701-4237-8299-c274f6ea819a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25644    0.51\n",
       "25656    0.92\n",
       "25675    0.48\n",
       "25729    2.40\n",
       "25730    0.56\n",
       "         ... \n",
       "36375    1.63\n",
       "36403    0.94\n",
       "36430    0.76\n",
       "36448    0.63\n",
       "36485    0.59\n",
       "Name: Prec, Length: 348, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "de4ae483-a56c-4735-a276-2088a19dbe68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25644    1970.0\n",
       "25656    1970.0\n",
       "25675    1970.0\n",
       "25729    1970.0\n",
       "25730    1970.0\n",
       "          ...  \n",
       "36375    1999.0\n",
       "36403    1999.0\n",
       "36430    1999.0\n",
       "36448    1999.0\n",
       "36485    1999.0\n",
       "Name: year, Length: 348, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b223c9e0-697e-41c5-a1ed-86945f2c498e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "333"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.array(block1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cdf19bbe-b6fd-4410-b2bb-350bdcb33b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contrast late period with early period, assuming a nonstationary fit\n",
    "# within each time period and finding RR based on midpoint of each period\n",
    "result = climextremes.calc_riskRatio_pot(\n",
    "    returnValue = 3,\n",
    "    \n",
    "    y1 = np.array(y1),\n",
    "    y2 = np.array(y2),\n",
    "    \n",
    "    x1 = earlyYears,\n",
    "    x2 = lateYears,\n",
    "    \n",
    "    threshold1 = threshold, \n",
    "    threshold2 = threshold,\n",
    "    \n",
    "    locationFun1 = 1, \n",
    "    locationFun2 = 1,\n",
    "    \n",
    "    xNew1 = int(earlyYears.mean()), \n",
    "    xNew2 = int(lateYears.mean()),\n",
    "    \n",
    "    blockIndex1 = np.array(block1),\n",
    "    blockIndex2 = np.array(block2),\n",
    "    \n",
    "    firstBlock1 = float(earlyYears[0]), # py2r sorunu buradan dolayı\n",
    "    firstBlock2 = float(lateYears[0]), # float kullanmak gerek\n",
    "    \n",
    "    ciType = 'lrt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4b23e70a-bdcf-44ca-af53-5f2eadfb8b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logRiskRatio': array([-0.68988299]),\n",
       " 'se_logRiskRatio': array([0.62212738]),\n",
       " 'riskRatio': array([0.50163476]),\n",
       " 'ci_riskRatio_lrt_names': array(['0.05', '0.95'], dtype='<U4'),\n",
       " 'ci_riskRatio_lrt': array([0.18263271, 1.33406702])}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e2f726-540d-4d94-8194-c3eebe961b8a",
   "metadata": {},
   "source": [
    "### 3.9 fit_gev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "da1de9a5-ad9f-4040-9c33-2f2eaac78498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function fit_gev:\n",
      "\n",
      "fit_gev(y=None, x=None, locationFun=None, scaleFun=None, shapeFun=None, nReplicates=1.0, replicateIndex=None, weights=None, returnPeriod=None, returnValue=None, getParams=False, getFit=False, xNew=None, xContrast=None, maxes=True, scaling=1.0, bootSE=False, bootControl=None, optimArgs=None, optimControl=None, missingFlag=None, initial=None, logScale=None, _normalizeX=True, _getInputs=False, _allowNoInt=True)\n",
      "    Fit a generalized extreme value model to block maxima or minima \n",
      "    \n",
      "    **description**\n",
      "    \n",
      "     Fit a generalized extreme value model, designed specifically for climate data. It includes options for variable weights (useful for local likelihood), as well as for bootstrapping to estimate uncertainties. Results can be returned in terms of parameter values, return values, return periods, return probabilities, and differences in either return values or log return probabilities (i.e., log risk ratios).\n",
      "    \n",
      "    **arguments**\n",
      "    \n",
      "    y: a numpy array of observed maxima or minima values. See ‘Details’ for how the values of ‘y’ should be ordered if there are multiple replicates and the values of ‘x’ are identical for all replicates. For better optimization performance, it is recommended that the ‘y’ have magnitude around one (see ‘Details’), for which one can use ‘scaling’.\n",
      "    \n",
      "    x: a numpy array or pandas data frame with columns corresponding to covariate/predictor/feature variables and each row containing the values of the variable for the corresponding observed maximum/minimum. The number of rows should either equal the length of ‘y’ or (if there is more than one replicate) it can optionally equal the number of observations in a single replicate, in which case the values will be assumed to be the same for all replicates. \n",
      "    \n",
      "    locationFun: numpy array of either character strings or indices describing a linear model (i.e., regression function) for the location parameter using columns from ‘x’. ‘x’ must be supplied if this is anything other than ‘None’.\n",
      "    \n",
      "    scaleFun: numpy array of either character strings or indices describing a linear model (i.e., regression function) for the log of the scale parameter using columns from ‘x’. ‘x’ must be supplied if this is anything other than ‘None’.\n",
      "    \n",
      "    shapeFun: numpy array of either character strings or indices describing a linear model (i.e., regression function) for the shape parameter using columns from ‘x’. ‘x’ must be supplied if this is anything other than ‘None’.\n",
      "    \n",
      "    nReplicates: numeric value indicating the number of replicates.\n",
      "    \n",
      "    replicateIndex: numpy array providing the index of the replicate corresponding to each element of ‘y’. Used (and therefore required) only when using bootstrapping with the resampling by replicates based on the ‘by’ element of ‘bootControl’.\n",
      "    \n",
      "    weights: a numpy array providing the weights for each observation. When there is only one replicate or the weights do not vary by replicate, an array of length equal to the number of observations. When weights vary by replicate, this should be of equal length to ‘y’. Likelihood contribution of each observation is multiplied by the corresponding weight. \n",
      "    \n",
      "    returnPeriod: numeric value giving the number of blocks for which return values should be calculated. For example a returnPeriod of 20 corresponds to the value of an event that occurs with probability 1/20 in any block and therefore occurs on average every 20 blocks. Often blocks will correspond to years.\n",
      "    \n",
      "    returnValue: numeric value giving the value for which return probabilities/periods should be calculated, where the period would be the average number of blocks until the value is exceeded and the probability the probability of exceeding the value in any single block.\n",
      "    \n",
      "    getParams: boolean indicating whether to return the fitted parameter values and their standard errors; WARNING: parameter values for models with covariates for the scale parameter must interpreted based on the value of ‘logScale’.\n",
      "    \n",
      "    getFit: boolean indicating whether to return the full fitted model (potentially useful for model evaluation and for understanding optimization problems); note that estimated parameters in the fit object for nonstationary models will not generally match the MLE provided when ‘getParams’ is ‘True’ because covariates are normalized before fitting and the fit object is based on the normalized covariates. Similarly, parameters will not match if ‘scaling’ is not 1. \n",
      "    \n",
      "    xNew: object of the same form as ‘x’, providing covariate/predictor/feature values for which return values/periods/probabilities are desired.\n",
      "    \n",
      "    xContrast: object of the same form and dimensions as ‘xNew’, providing covariate/predictor/feature values for which to calculate the differences of the return values and/or log return probabilities relative to the values in ‘xNew’. This provides a way to estimate the difference in return value or log return probabilities (i.e., log risk ratios).\n",
      "    \n",
      "    maxes: boolean indicating whether analysis is for block maxima (‘True’) or block minima (‘False’); in the latter case, the function works with the negative of the values, changing the sign of the resulting location parameters\n",
      "    \n",
      "    scaling: positive-valued number used to scale the data values for more robust optimization performance. When multiplied by the values, it should produce values with magnitude around 1.\n",
      "    \n",
      "    bootSE: boolean indicating whether to use the bootstrap to estimate standard errors.\n",
      "    \n",
      "    bootControl: a dictionary of control parameters for the bootstrapping. See ‘Details’.\n",
      "    \n",
      "    optimArgs: a dictionary with named components matching exactly any arguments that the user wishes to pass to R's ‘optim’ function. See ‘help(optim)’ in R for details. Of particular note, ‘'method'’ can be used to choose the optimization method used for maximizing the log-likelihood to fit the model (e.g., ‘'method'’ could be 'BFGS' instead of the default 'Nelder-Mead'). To specify the ‘control’ argument, use ‘optimControl’ rather than including ‘control’ here.\n",
      "    \n",
      "    optimControl: a dictionary with named components matching exactly any elements that the user wishes to pass as the ‘control’ argument to R's ‘optim’ function. For example, ‘control={'maxit': VALUE)’ for a user-specified VALUE can be used to increase the number of iterations if the optimization is converging slowly.\n",
      "    \n",
      "    missingFlag: optional value to be interpreted as missing values (instead of default ‘numpy.nan’)\n",
      "    \n",
      "    initial: a dictionary with components named ‘'location'’, ‘'scale'’, and ‘'shape'’ providing initial parameter values, intended for use in speeding up or enabling optimization when the default initial values are resulting in failure of the optimization; note that use of ‘scaling’, ‘logScale’, and ‘.normalizeX = True’ cause numerical changes in some of the parameters. For example with ‘logScale = True’, initial value(s) for ‘'scale'’ should be specified on the log scale.\n",
      "    \n",
      "    logScale: boolean indicating whether optimization for the scale parameter should be done on the log scale. By default this is ‘False’ when the scale is not a function of covariates and ‘True’ when the scale is a function of covariates (to ensure the scale is positive regardless of the regression coefficients). \n",
      "    \n",
      "    .normalizeX: boolean indicating whether to normalize ‘x’ values for better numerical performance; default is ‘True’.\n",
      "    \n",
      "    .getInputs: boolean indicating whether to return intermediate objects used in fitting. Defaults to ‘False’ and intended for internal use only\n",
      "    \n",
      "    **details**\n",
      "    \n",
      "     This function allows one to fit stationary or nonstationary block maxima/minima models using the generalized extreme value distribution. The function can return parameter estimates, return value/level for a given return period (number of blocks), and return probabilities/periods for a given return value/level. The function provides standard errors based on the usual MLE asymptotics, with delta-method-based standard errors for functionals of the parameters, but also standard errors based on the nonparametric bootstrap, either resampling by block or by replicate or both.\n",
      "    \n",
      "     Replicates:\n",
      "    \n",
      "     Replicates are repeated datasets, each with the same structure, including the same number of block maxima/minima. The additional observations in multiple replicates could simply be treated as additional blocks without replication (see next paragraph), but when the covariate values and' weights 'are the same across replicates, it is simpler to make use of ' nReplicates ' and ' replicateIndex '.\n",
      "    \n",
      "     When using multiple replicates (e.g., multiple members of a climate model' initial 'condition ensemble), the standard input format is to append observations for additional replicates to the ' y ' argument and indicate the replicate ID for each value via ' replicateIndex ', which would be of the form 1,1,1,...2,2,2,...3,3,3,... etc. The values for each replicate should be grouped together and in the same order within replicate so that ' x ' can be correctly matched to the ' y ' values when ' x ' is only supplied for the first replicate. In other words, ' y ' should first contain all the values for the first replicate, then all the values for the second replicate in the same block order as for the first replicate, and so forth. Note that if ' y ' is provided as a matrix with the number of rows equal to the number of observations in each replicate and the columns corresponding to replicates, this ordering will occur naturally.\n",
      "    \n",
      "     However, if one has different covariate values for different replicates, then one needs to treat the additional replicates as providing additional blocks, with only a single replicate (and ' nReplicates ' set to 1). The covariate values can then be included as additional rows in ' x '. Similarly, if there is a varying number of replicates by block, then all block-replicate pairs should be treated as individual blocks with a corresponding row in ' x ' (and ' nReplicates ' set to 1).\n",
      "    \n",
      "    ' bootControl ' arguments:\n",
      "    \n",
      "     The ' bootControl ' argument is a list (or dictionary when calling from Python) that can supply any of the following components:\n",
      "    \n",
      "      seed. Value of the random number seed as a single value, or in the form of  .Random.seed , to set before doing resampling. Defaults to  1 .\n",
      "      n. Number of bootstrap samples. Defaults to  250 .\n",
      "      by. Character string, one of  'block' ,  'replicate' , or  'joint' , indicating the basis for the resampling. If  'block' , resampled datasets will consist of blocks drawn at random from the original set of blocks; if there are replicates, each replicate will occur once for every resampled block. If  'replicate' , resampled datasets will consist of replicates drawn at random from the original set of replicates; all blocks from a replicate will occur in each resampled replicate. Note that this preserves any dependence across blocks rather than assuming independence between blocks. If  'joint'  resampled datasets will consist of block-replicate pairs drawn at random from the original set of block-replicate pairs. Defaults to  'block' . \n",
      "      getSample. Logical/boolean indicating whether the user wants the full bootstrap sample of parameter estimates and/or return value/period/probability information returned for use in subsequent calculations; if False (the default), only the bootstrap-based estimated standard errors are returned.\n",
      "    \n",
      "     \n",
      "     Optimization failures:\n",
      "    \n",
      "     It is not uncommon for maximization of the log-likelihood to fail for extreme value models. Users should carefully check the  info  element of the return object to ensure that the optimization converged. For better optimization performance, it is recommended that the observations be scaled to have magnitude around one (e.g., converting precipitation from mm to cm). When there is a convergence failure, one can try a different optimization method, more iterations, or different starting values -- see ' optimArgs ' and ' initial '. In particular, the Nelder-Mead method is used; users may want to try the BFGS method by setting ' optimArgs '= list(method = 'BFGS')  (or ' optimArgs '= {'method': 'BFGS'}  when calling from Python). \n",
      "    \n",
      "     When using the bootstrap, users should check that the number of convergence failures when fitting to the boostrapped datasets is small, as it is not clear how to interpret the bootstrap results when there are convergence failures for some bootstrapped datasets.\n",
      "    \n",
      "    **value**\n",
      "    \n",
      "    The primary outputs of this function are as follows, depending on what is requested via ‘returnPeriod’, ‘returnValue’, ‘getParams’ and ‘xContrast’: \n",
      "    \n",
      "     when ‘returnPeriod’ is given: for the period given in ‘returnPeriod’ the return value(s) (‘returnValue’) and its corresponding asymptotic standard error (‘se_returnValue’) and, when ‘bootSE=True’, also the bootstrapped standard error (‘se_returnValue_boot’). For nonstationary models, these correspond to the covariate values given in ‘x’. \n",
      "    \n",
      "     when ‘returnValue’ is given: for the value given in ‘returnValue’, the log exceedance probability (‘logReturnProb’) and the corresponding asymptotic standard error (‘se_logReturnProb’) and, when ‘bootSE=True’, also the bootstrapped standard error (‘se_logReturnProb_boot’). This exceedance probability is the probability of exceedance for a single block. Also returned are the log return period (‘logReturnPeriod’) and its corresponding asymptotic standard error (‘se_logReturnPeriod’) and, when ‘bootSE=True’, also the bootstrapped standard error (‘se_logReturnPeriod_boot’). For nonstationary models, these correspond to the covariate values given in ‘x’. Note that results are on the log scale as probabilities and return times are likely to be closer to normally distributed on the log scale and therefore standard errors are more naturally given on this scale. Confidence intervals for return probabilities/periods can be obtained by exponentiating the interval obtained from plus/minus twice the standard error of the log probabilities/periods. \n",
      "    \n",
      "     when ‘getParams=True’: the MLE for the model parameters (‘mle’) and corresponding asymptotic standard error (‘se_mle’) and, when ‘bootSE=True’, also the bootstrapped standard error (‘se_mle_boot’). \n",
      "    \n",
      "     when ‘xContrast’ is specified for nonstationary models: the difference in return values (‘returnValueDiff’) and its corresponding asymptotic standard error (‘se_returnValueDiff’) and, when ‘bootSE=True’, bootstrapped standard error (‘se_returnValueDiff_boot’). These differences correspond to the differences when contrasting each row in ‘x’ with the corresponding row in ‘xContrast’. Also returned are the difference in log return probabilities (i.e., the log risk ratio) (‘logReturnProbDiff’) and its corresponding asymptotic standard error (‘se_logReturnProbDiff’) and, when ‘bootSE=True’, bootstrapped standard error (‘se_logReturnProbDiff_boot’).\n",
      "    \n",
      "    **author**\n",
      "    \n",
      "     Christopher J. Paciorek\n",
      "    \n",
      "    **references**\n",
      "    \n",
      "     Coles, S. 2001. An Introduction to Statistical Modeling of Extreme Values. Springer.\n",
      "    \n",
      "     Paciorek, C.J., D.A. Stone, and M.F. Wehner. 2018. Quantifying uncertainty in the attribution of human influence on severe weather. Weather and Climate Extremes 20:69-80. arXiv preprint <https://arxiv.org/abs/1706.03388>.\n",
      "    \n",
      "    **examples**\n",
      "    \n",
      "    >>> Fort = climextremes.Fort\n",
      "    ... \n",
      "    ... FortMax = Fort.groupby('year').max()[['Prec']]\n",
      "    ... FortMax.reset_index(inplace=True)\n",
      "    ... \n",
      "    ... # stationary fit\n",
      "    ... result = climextremes.fit_gev(numpy.array(FortMax.Prec), returnPeriod = 20, returnValue = 3.5, getParams = True, bootSE = True)\n",
      "    ... result['returnValue']\n",
      "    ... result['se_returnValue']       # return value standard error (asymptotic)\n",
      "    ... result['se_returnValue_boot']  # return value standard error (bootstrapping)\n",
      "    ... result['logReturnProb']        # log of probability of exceeding 'returnValue'\n",
      "    ... result['mle']                  # MLE array \n",
      "    ... result['mle_names']            # names for MLE array \n",
      "    ... result['mle'][2]               # MLE for shape parameter\n",
      "    ... \n",
      "    ... result['numBootFailures']      # number of bootstrap datasets for which the model could not be fit; if this is non-negligible relative to the number of bootstrap samples (default of 250), interpret the bootstrap results with caution\n",
      "    ... \n",
      "    ... # modifying the bootstrapping specifications\n",
      "    ... result = climextremes.fit_gev(numpy.array(FortMax.Prec), returnPeriod = 20, returnValue = 3.5, getParams = True, bootSE = True, bootControl = {'n': 100, 'seed': 3})\n",
      "    ... result['se_returnValue_boot']  # return value standard error (bootstrapping)\n",
      "    ... \n",
      "    ... yrsToPred = numpy.array([min(Fort.year), max(Fort.year)])\n",
      "    ... \n",
      "    ... # nonstationary fit with location linear in year and two return values requested\n",
      "    ... result_ns = climextremes.fit_gev(numpy.array(FortMax.Prec), numpy.array(FortMax.year), locationFun = 1, returnPeriod = numpy.array([20, 30]), returnValue = 3.5, xNew = yrsToPred, getParams = True, bootSE = False)\n",
      "    ... result_ns['returnValue']\n",
      "    ... result_ns['se_returnValue']\n",
      "    ... \n",
      "    ... \n",
      "    ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(climextremes.fit_gev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b94aede-995a-4f4b-8c75-60b80570b178",
   "metadata": {},
   "source": [
    "#### 3.9.1 Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "52daa9c0-3b81-48af-b628-eb75780ae0dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mle_names': array(['location', 'scale', 'shape'], dtype='<U8'),\n",
       " 'mle': array([1.34672337, 0.53281954, 0.17366235]),\n",
       " 'se_mle_names': array(['location', 'scale', 'shape'], dtype='<U8'),\n",
       " 'se_mle': array([0.06169221, 0.04878825, 0.09195978]),\n",
       " 'nllh': array([104.96453549]),\n",
       " 'returnValue': array([3.41770382]),\n",
       " 'se_returnValue': array([0.33295171]),\n",
       " 'logReturnProb': array([-3.08498961]),\n",
       " 'se_logReturnProb': array([0.38110335]),\n",
       " 'logReturnPeriod': array([3.08498961]),\n",
       " 'se_logReturnPeriod': array([0.38110335]),\n",
       " 'info': {'convergence': array([0], dtype=int32),\n",
       "  'counts_names': array(['function', 'gradient'], dtype='<U8'),\n",
       "  'counts': array([80., nan]),\n",
       "  'message': None,\n",
       "  'failure': array([0], dtype=int32)},\n",
       " 'se_mle_boot_names': array(['location', 'scale', 'shape'], dtype='<U8'),\n",
       " 'se_mle_boot': array([0.06283165, 0.04521626, 0.07946007]),\n",
       " 'se_returnValue_boot_names': array(['20'], dtype='<U2'),\n",
       " 'se_returnValue_boot': array([0.27335734]),\n",
       " 'se_logReturnProb_boot_names': array(['3.5'], dtype='<U3'),\n",
       " 'se_logReturnProb_boot': array([0.33902666]),\n",
       " 'se_logReturnPeriod_boot_names': array(['3.5'], dtype='<U3'),\n",
       " 'se_logReturnPeriod_boot': array([0.33902666]),\n",
       " 'numBootFailures': array([0.])}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stationary\n",
    "dtMax = dt.groupby('year').max()[['Prec']]\n",
    "dtMax.reset_index(inplace=True)\n",
    "\n",
    "# stationary fit\n",
    "result = climextremes.fit_gev(np.array(dtMax.Prec),\n",
    "                              returnPeriod = 20,\n",
    "                              returnValue = 3.5,\n",
    "                              getParams = True,\n",
    "                              bootSE = True)\n",
    "\n",
    "#print(result['returnValue'])\n",
    "#print(result['se_returnValue']  )     # return value standard error (asymptotic)\n",
    "#print(result['se_returnValue_boot'])  # return value standard error (bootstrapping)\n",
    "#print(result['logReturnProb'])        # log of probability of exceeding 'returnValue'\n",
    "#print(result['mle'])                  # MLE array \n",
    "#print(result['mle_names'])           # names for MLE array \n",
    "#print(result['mle'][2] )              # MLE for shape parameter\n",
    "#print(result['numBootFailures'])      # number of bootstrap datasets for which the model could not be fit; if this is non-negligible relative to the number of bootstrap samples (default of 250), interpret the bootstrap results with caution)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4b69b2fd-2425-4172-932d-e772b77fbc4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mle_names': array(['location', 'scale', 'shape'], dtype='<U8'),\n",
       " 'mle': array([1.34672337, 0.53281954, 0.17366235]),\n",
       " 'se_mle_names': array(['location', 'scale', 'shape'], dtype='<U8'),\n",
       " 'se_mle': array([0.06169221, 0.04878825, 0.09195978]),\n",
       " 'nllh': array([104.96453549]),\n",
       " 'returnValue': array([3.41770382]),\n",
       " 'se_returnValue': array([0.33295171]),\n",
       " 'logReturnProb': array([-3.08498961]),\n",
       " 'se_logReturnProb': array([0.38110335]),\n",
       " 'logReturnPeriod': array([3.08498961]),\n",
       " 'se_logReturnPeriod': array([0.38110335]),\n",
       " 'info': {'convergence': array([0], dtype=int32),\n",
       "  'counts_names': array(['function', 'gradient'], dtype='<U8'),\n",
       "  'counts': array([80., nan]),\n",
       "  'message': None,\n",
       "  'failure': array([0], dtype=int32)},\n",
       " 'se_mle_boot_names': array(['location', 'scale', 'shape'], dtype='<U8'),\n",
       " 'se_mle_boot': array([0.06449165, 0.04783417, 0.07962154]),\n",
       " 'se_returnValue_boot_names': array(['20'], dtype='<U2'),\n",
       " 'se_returnValue_boot': array([0.27289224]),\n",
       " 'se_logReturnProb_boot_names': array(['3.5'], dtype='<U3'),\n",
       " 'se_logReturnProb_boot': array([0.40433406]),\n",
       " 'se_logReturnPeriod_boot_names': array(['3.5'], dtype='<U3'),\n",
       " 'se_logReturnPeriod_boot': array([0.40433406]),\n",
       " 'numBootFailures': array([0.])}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# varying bootstrap input\n",
    "#modifying the bootstrapping specifications\n",
    "\n",
    "result = climextremes.fit_gev(np.array(dtMax.Prec),\n",
    "                              returnPeriod = 20,\n",
    "                              returnValue = 3.5,\n",
    "                              getParams = True,\n",
    "                              bootSE = True,\n",
    "                              bootControl = {'n': 100, 'seed': 3})\n",
    "#result['se_returnValue_boot']  # return value standard error (bootstrapping)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c1e398ec-ed12-4316-96c0-670579c92c25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mle_names': array(['mu0', 'mu1', 'scale', 'shape'], dtype='<U5'),\n",
       " 'mle': array([-0.02923163,  0.00070603,  0.53260967,  0.17318927]),\n",
       " 'se_mle_names': array(['mu0', 'mu1', 'scale', 'shape'], dtype='<U5'),\n",
       " 'se_mle': array([3.70197889e+00, 1.89950180e-03, 4.88358114e-02, 9.24635033e-02]),\n",
       " 'nllh': array([104.89492646]),\n",
       " 'returnValue_column_names': array(['20', '30'], dtype='<U2'),\n",
       " 'returnValue': array([[3.38080458, 3.76326235],\n",
       "        [3.45070116, 3.83315893]]),\n",
       " 'se_returnValue_column_names': array(['20', '30'], dtype='<U2'),\n",
       " 'se_returnValue': array([[0.34737996, 0.44809332],\n",
       "        [0.34446572, 0.44446079]]),\n",
       " 'logReturnProb': array([-3.12485074, -3.04944399]),\n",
       " 'se_logReturnProb': array([0.40434473, 0.38671426]),\n",
       " 'logReturnPeriod': array([3.12485074, 3.04944399]),\n",
       " 'se_logReturnPeriod': array([0.40434473, 0.38671426]),\n",
       " 'info': {'convergence': array([0], dtype=int32),\n",
       "  'counts_names': array(['function', 'gradient'], dtype='<U8'),\n",
       "  'counts': array([187.,  nan]),\n",
       "  'message': None,\n",
       "  'failure': array([0], dtype=int32)}}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# non-stationary with linear time-varying location parameter\n",
    "# nonstationary fit with location linear in year and two return values requested\n",
    "\n",
    "yrsToPred = np.array([min(dt.year), max(dt.year)])\n",
    "\n",
    "result_ns = climextremes.fit_gev(np.array(dtMax.Prec),\n",
    "                                 np.array(dtMax.year),\n",
    "                                 locationFun = 1,\n",
    "                                 returnPeriod = np.array([20, 30]),\n",
    "                                 returnValue = 3.5,\n",
    "                                 xNew = yrsToPred,\n",
    "                                 getParams = True,\n",
    "                                 bootSE = False)\n",
    "result_ns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d88fc2-bef4-4f33-b2d4-973c30515532",
   "metadata": {},
   "source": [
    "### 3.10 fit_pot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "931a94c1-2e90-419d-9906-acf23ea531de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function fit_pot:\n",
      "\n",
      "fit_pot(y=None, x=None, threshold=None, locationFun=None, scaleFun=None, shapeFun=None, nBlocks=None, blockIndex=None, firstBlock=1.0, index=None, nReplicates=1.0, replicateIndex=None, weights=None, proportionMissing=None, returnPeriod=None, returnValue=None, getParams=False, getFit=False, xNew=None, xContrast=None, declustering=None, upperTail=True, scaling=1.0, bootSE=False, bootControl=None, optimArgs=None, optimControl=None, initial=None, logScale=None, _normalizeX=True, _getInputs=False, _allowNoInt=True)\n",
      "    Fit a peaks-over-threshold model to exceedances over a threshold \n",
      "    \n",
      "    **description**\n",
      "    \n",
      "     Fit a peaks-over-threshold model, designed specifically for climate data, to exceedance-only data, using the point process approach. Any covariates/predictors/features assumed to vary only between and not within blocks of observations. It includes options for variable weights (useful for local likelihood) and variable proportions of missing data, as well as for bootstrapping to estimate uncertainties. Results can be returned in terms of parameter values, return values, return periods, return probabilities, and differences in either return values or log return probabilities (i.e., log risk ratios).\n",
      "    \n",
      "    **arguments**\n",
      "    \n",
      "    y: a numpy array of exceedance values (values of the outcome variable above the threshold). For better optimization performance, it is recommended that the ‘y’ have magnitude around one (see ‘Details’), for which one can use ‘scaling’.\n",
      "    \n",
      "    x: a numpy array or pandas data frame with columns corresponding to covariate/predictor/feature variables and each row containing the values of the variable for a block (e.g., often a year with climate data). The number of rows must equal the number of blocks.\n",
      "    \n",
      "    threshold: a single numeric value for constant threshold or a numpy array with length equal to the number of blocks, indicating the threshold for each block.\n",
      "    \n",
      "    locationFun: numpy array of either character strings or indices describing a linear model (i.e., regression function) for the location parameter using columns from ‘x’. ‘x’ must be supplied if this is anything other than ‘None’.\n",
      "    \n",
      "    scaleFun: numpy array of either character strings or indices describing a linear model (i.e., regression function) for the log of the scale parameter using columns from ‘x’. ‘x’ must be supplied if this is anything other than ‘None’.\n",
      "    \n",
      "    shapeFun: numpy array of either character strings or indices describing a linear model (i.e., regression function) for the shape parameter using columns from ‘x’. ‘x’ must be supplied if this is anything other than ‘None’.\n",
      "    \n",
      "    nBlocks: number of blocks (e.g., a block will often be a year with climate data); note this value determines the interpretation of return values/periods/probabilities; see ‘returnPeriod’ and ‘returnValue’.\n",
      "    \n",
      "    blockIndex: numpy array providing the index of the block corresponding to each element of ‘y’. Used only when ‘x’ is provided to match exceedances to the covariate/predictor/feature value for the exceedance or when using bootstrapping with the resampling based on blocks based on the ‘by’ element of ‘bootControl’. If ‘firstBlock’ is not equal to one, then ‘blockIndex’ need not have one as its smallest possible value.\n",
      "    \n",
      "    firstBlock: single numeric value indicating the numeric value of the first possible block of ‘blockIndex’. For example the values in ‘blockIndex’ might indicate the year of each exceedance with the first year of data being 1969, in which case ‘firstBlock’ would be 1969. Note that the first block may not have any exceedances so it may not be represented in ‘blockIndex’. Used only to adjust ‘blockIndex’ so that the block indices start at one and therefore correspond to the rows of ‘x’.\n",
      "    \n",
      "    index: (optional) numpy array providing the integer-valued index (e.g., julian day for daily climate data) corresponding to each element of ‘y’. For example if there are 10 original observations and the third, fourth, and seventh values are exceedances, then ‘index’ would be the vector 3,4,7. Used only when ‘declustering’ is provided to determine which exceedances occur sequentially or within a contiguous set of values of a given length. The actual values are arbitrary; only the lags between the values are used.\n",
      "    \n",
      "    nReplicates: numeric value indicating the number of replicates.\n",
      "    \n",
      "    replicateIndex: numpy array providing the index of the replicate corresponding to each element of ‘y’. Used for three purposes: (1) when using bootstrapping with the resampling based on replicates based on the ‘by’ element of ‘bootControl’, (2) to avoid treating values in different replicates as potentially being sequential or within a short interval when removing values based on ‘declustering’, and (3) to match outcomes to ‘weights’ or ‘proportionMissing’ when either vary by replicate.  \n",
      "    \n",
      "    weights: a numpy array providing the weights by block. When there is only one replicate or the weights do not vary by replicate, a one-dimensional array of length equal to the number of blocks. When weights vary by replicate, a two-dimensional array with rows corresponding to blocks and columns to replicates. Likelihood contribution of each block is multiplied by the corresponding weight. \n",
      "    \n",
      "    proportionMissing: a numeric value or numpy array indicating the proportion of missing values in the original dataset before exceedances were selected. When the proportion missing is the same for all blocks and replicates, a single value. When there is only one replicate or the weights do not vary by replicate, a one-dimensional array of length equal to the number of blocks. When weights vary by replicate, a two-dimensional array with rows corresponding to blocks and columns to replicates.\n",
      "    \n",
      "    returnPeriod: numeric value giving the number of blocks for which return values should be calculated. For example a ‘returnPeriod’ equal to 20 will result in calculation of the value of an event that occurs with probability 1/20 in any block and therefore occurs on average every 20 blocks. Often blocks will correspond to years.\n",
      "    \n",
      "    returnValue: numeric value giving the value for which return probabilities/periods should be calculated, where the resulting period will be the average number of blocks until the value is exceeded and the probability the probability of exceeding the value in any single block.\n",
      "    \n",
      "    getParams: boolean indicating whether to return the fitted parameter values and their standard errors; WARNING: parameter values for models with covariates for the scale parameter must interpreted based on the value of ‘logScale’.\n",
      "    \n",
      "    getFit: boolean indicating whether to return the full fitted model (potentially useful for model evaluation and for understanding optimization problems); note that estimated parameters in the fit object for nonstationary models will not generally match the MLE provided when ‘getParams’ is ‘True’ because covariates are normalized before fitting and the fit object is based on the normalized covariates. Similarly, parameters will not match if ‘scaling’ is not 1. \n",
      "    \n",
      "    xNew: object of the same form as ‘x’, providing covariate/predictor/feature values for which return values/periods/probabilities are desired.\n",
      "    \n",
      "    xContrast: object of the same form and dimensions as ‘xNew’, providing covariate/predictor/feature values for which to calculate the differences of the return values and/or log return probabilities relative to the values in ‘xNew’. This provides a way to estimate differences in return value or log return probabilities (i.e., log risk ratios).\n",
      "    \n",
      "    declustering: one of ‘None’, ‘'noruns'’, or a number. If ‘'noruns'’ is specified, only the maximum (or minimum if ‘upperTail = False’) value within a set of exceedances corresponding to successive indices is included. If a number, this should indicate the size of the interval (which will be used with the ‘index’ argument) within which to allow only the largest (or smallest if ‘upperTail = False’) value.\n",
      "    \n",
      "    upperTail: boolean indicating whether one is working with exceedances over a high threshold (‘True’) or exceedances under a low threshold (‘False’); in the latter case, the function works with the negative of the values and the threshold, changing the sign of the resulting location parameters.\n",
      "    \n",
      "    scaling: positive-valued scalar used to scale the data values for more robust optimization performance. When multiplied by the values, it should produce values with magnitude around 1.\n",
      "    \n",
      "    bootSE: boolean indicating whether to use the bootstrap to estimate standard errors.\n",
      "    \n",
      "    bootControl: a dictionary of control parameters for the bootstrapping. See ‘Details’.\n",
      "    \n",
      "    optimArgs: a dictionary with named components matching exactly any arguments that the user wishes to pass to R's ‘optim’ function. See ‘help(optim)’ in R for details. Of particular note, ‘'method'’ can be used to choose the optimization method used for maximizing the log-likelihood to fit the model (e.g., ‘'method'’ could be 'BFGS' instead of the default 'Nelder-Mead'). To specify the ‘control’ argument, use ‘optimControl’ rather than including ‘control’ here.\n",
      "    \n",
      "    optimControl: a dictionary with named components matching exactly any elements that the user wishes to pass as the ‘control’ argument to R's ‘optim’ function. For example, ‘control={'maxit': VALUE)’ for a user-specified VALUE can be used to increase the number of iterations if the optimization is converging slowly.\n",
      "    \n",
      "    initial: a dictionary with components named ‘'location'’, ‘'scale'’, and ‘'shape'’ providing initial parameter values, intended for use in speeding up or enabling optimization when the default initial values are resulting in failure of the optimization; note that use of ‘scaling’, ‘logScale’ and ‘.normalizeX = True’ cause numerical changes in some of the parameters. For example with ‘logScale = True’, initial value(s) for ‘'scale'’ should be specified on the log scale.\n",
      "    \n",
      "    logScale: boolean indicating whether optimization for the scale parameter should be done on the log scale. By default this is ‘False’ when the scale is not a function of covariates and ‘True’ when the scale is a function of covariates (to ensure the scale is positive regardless of the regression coefficients). \n",
      "    \n",
      "    .normalizeX: boolean indicating whether to normalize 'x' values for better numerical performance; default is ‘True’.\n",
      "    \n",
      "    .getInputs: boolean indicating whether to return intermediate objects used in fitting. Defaults to ‘False’ and intended for internal use only\n",
      "    \n",
      "    **details**\n",
      "    \n",
      "     This function allows one to fit stationary or nonstationary peaks-over-threshold models using the point process approach. The function can return parameter estimates (which are asymptotically equivalent to GEV model parameters for block maxima data), return value/level for a given return period (number of blocks),  and return probabilities/periods for a given return value/level. The function provides standard errors based on the usual MLE asymptotics, with delta-method-based standard errors for functionals of the parameters, but also standard errors based on the nonparametric bootstrap, either resampling by block or by replicate or both.\n",
      "    \n",
      "     Analyzing aggregated observations, such as yearly averages:\n",
      "    \n",
      "     Aggregated average or summed data such as yearly or seasonal averages can be fit using this function. The best way to do this is to specify ' nBlocks ' to be the number of observations (i.e., the length of the observation period, not the number of exceedances). Then any return probabilities can be interpreted as the probabilities for a single block (e.g., a year). If instead ' nBlocks ' were one (i.e., a single block) then probabilities would be interpreted as the probability of occurrence in a multi-year block. \n",
      "    \n",
      "     Blocks and replicates:\n",
      "    \n",
      "     Note that blocks and replicates are related concepts. Blocks are the grouping of values such that return values and return periods are calculated based on the equivalent block maxima (or minima) generalized extreme value model. For example if a block is a year, the return period is the average number of years before the given value is seen, while the return value when ' returnPeriod ' is, say, 20, is the value exceeded on average once in 20 years. A given dataset will generally have multiple blocks. In some cases a block may contain only a single value, such as when analyzing yearly sums or averages.\n",
      "    \n",
      "     Replicates are repeated datasets, each with the same structure, including the same number of blocks. The additional blocks in multiple replicates could simply be treated as additional blocks without replication, but when the predictor variables and' weights 'are the same across replicates, it is simpler to make use of ' nReplicates ' and ' replicateIndex ' (see next paragraph). A given replicate might only contain a single block, such as with an ensemble of short climate model runs that are run only for the length of a single block (e.g., a single year). In this case ' nBlocks ' should be set to one.\n",
      "    \n",
      "     When using multiple replicates (e.g., multiple members of a climate model' initial 'condition ensemble), the standard input format is to append outcome values for additional replicates to the ' y ' argument and indicate the replicate ID for each exceedance in ' replicateIndex '. However, if one has different covariate values or thresholds for different replicates, then one needs to treat the additional replicates as providing additional blocks, with only a single replicate. The covariate values can then be included as additional rows in ' x ', with ' nBlocks ' reflecting the product of the number of blocks per replicate and the number of replicates and ' nReplicates ' set to 1. In this situation, if ' declustering ' is specified, make sure to set ' index ' such that the values for separate replicates do not overlap with each other, to avoid treating exceedances from different replicates as being sequential or from a contiguous set of values. Similarly, if there is a varying number of replicates by block, then all block-replicate pairs should be treated as individual blocks with a corresponding row in ' x '.\n",
      "    \n",
      "    ' bootControl ' arguments:\n",
      "    \n",
      "     The ' bootControl ' argument is a list (or dictionary when calling from Python) that can supply any of the following components:\n",
      "    \n",
      "      seed. Value of the random number seed as a single value, or in the form of  .Random.seed , to set before doing resampling. Defaults to  1 .\n",
      "      n. Number of bootstrap samples. Defaults to  250 .\n",
      "      by. Character string, one of  'block' ,  'replicate' , or  'joint' , indicating the basis for the resampling. If  'block' , resampled datasets will consist of blocks drawn at random from the original set of blocks; if there are replicates, each replicate will occur once for every resampled block. If  'replicate' , resampled datasets will consist of replicates drawn at random from the original set of replicates; all blocks from a replicate will occur in each resampled replicate. Note that this preserves any dependence across blocks rather than assuming independence between blocks. If  'joint'  resampled datasets will consist of block-replicate pairs drawn at random from the original set of block-replicate pairs. Defaults to  'block' . \n",
      "      getSample. Logical/boolean indicating whether the user wants the full bootstrap sample of parameter estimates and/or return value/period/probability information provided for use in subsequent calculations; if False (the default), only the bootstrap-based estimated standard errors are returned.\n",
      "    \n",
      "     \n",
      "     Optimization failures:\n",
      "    \n",
      "     It is not uncommon for maximization of the log-likelihood to fail for extreme value models. Users should carefully check the  info  element of the return object to ensure that the optimization converged. For better optimization performance, it is recommended that the observations be scaled to have magnitude around one (e.g., converting precipitation from mm to cm). When there is a convergence failure, one can try a different optimization method, more iterations, or different starting values -- see ' optimArgs ' and ' initial '. In particular, the Nelder-Mead method is used; users may want to try the BFGS method by setting ' optimArgs '= list(method = 'BFGS')  (or ' optimArgs '= {'method': 'BFGS'}  if calling from Python).\n",
      "    \n",
      "     When using the bootstrap, users should check that the number of convergence failures when fitting to the boostrapped datasets is small, as it is not clear how to interpret the bootstrap results when there are convergence failures for some bootstrapped datasets.\n",
      "    \n",
      "    **value**\n",
      "    \n",
      "    The primary outputs of this function are as follows, depending on what is requested via ‘returnPeriod’, ‘returnValue’, ‘getParams’ and ‘xContrast’: \n",
      "    \n",
      "     when ‘returnPeriod’ is given: for the period given in ‘returnPeriod’ the return value(s) (‘returnValue’) and its corresponding asymptotic standard error (‘se_returnValue’) and, when ‘bootSE=True’, also the bootstrapped standard error (‘se_returnValue_boot’). For nonstationary models, these correspond to the covariate values given in ‘x’. \n",
      "    \n",
      "     when ‘returnValue’ is given: for the value given in ‘returnValue’, the log exceedance probability (‘logReturnProb’) and the corresponding asymptotic standard error (‘se_logReturnProb’) and, when ‘bootSE=True’, also the bootstrapped standard error (‘se_logReturnProb_boot’). This exceedance probability is the probability of exceedance for a single block. Also returned are the log return period (‘logReturnPeriod’) and its corresponding asymptotic standard error (‘se_logReturnPeriod’) and, when ‘bootSE=True’, also the bootstrapped standard error (‘se_logReturnPeriod_boot’). For nonstationary models, these correspond to the covariate values given in ‘x’. Note that results are on the log scale as probabilities and return times are likely to be closer to normally distributed on the log scale and therefore standard errors are more naturally given on this scale. Confidence intervals for return probabilities/periods can be obtained by exponentiating the interval obtained from plus/minus twice the standard error of the log probabilities/periods. \n",
      "    \n",
      "     when ‘getParams=True’: the MLE for the model parameters (‘mle’) and corresponding asymptotic standard error (‘se_mle’) and, when ‘bootSE=True’, also the bootstrapped standard error (‘se_mle_boot’). \n",
      "    \n",
      "     when ‘xContrast’ is specified for nonstationary models: the difference in return values (‘returnValueDiff’) and its corresponding asymptotic standard error (‘se_returnValueDiff’) and, when ‘bootSE=True’, bootstrapped standard error (‘se_returnValueDiff_boot’). These differences correspond to the differences when contrasting each row in ‘x’ with the corresponding row in ‘xContrast’. Also returned are the difference in log return probabilities (i.e., the log risk ratio) (‘logReturnProbDiff’) and its corresponding asymptotic standard error (‘se_logReturnProbDiff’) and, when ‘bootSE=True’, bootstrapped standard error (‘se_logReturnProbDiff_boot’).\n",
      "    \n",
      "    **author**\n",
      "    \n",
      "     Christopher J. Paciorek\n",
      "    \n",
      "    **references**\n",
      "    \n",
      "     Coles, S. 2001. An Introduction to Statistical Modeling of Extreme Values. Springer.\n",
      "    \n",
      "     Paciorek, C.J., D.A. Stone, and M.F. Wehner. 2018. Quantifying uncertainty in the attribution of human influence on severe weather. Weather and Climate Extremes 20:69-80. arXiv preprint <https://arxiv.org/abs/1706.03388>.\n",
      "    \n",
      "    **examples**\n",
      "    \n",
      "    >>> Fort = climextremes.Fort\n",
      "    ... \n",
      "    ... firstYr = min(Fort.year)\n",
      "    ... yrs = numpy.array(range(int(firstYr), int(max(Fort.year)+1)))\n",
      "    ... nYrs = len(yrs)\n",
      "    ... yrsToPred = numpy.array([min(Fort.year), max(Fort.year)])\n",
      "    ... \n",
      "    ... threshold = 0.395\n",
      "    ... \n",
      "    ... FortExc = Fort[Fort.Prec > threshold]\n",
      "    ... \n",
      "    ... # stationary fit\n",
      "    ... result = climextremes.fit_pot(numpy.array(FortExc.Prec), nBlocks = nYrs, threshold = threshold, firstBlock = firstYr, blockIndex = numpy.array(FortExc.year), getParams = True, returnPeriod = 20, returnValue = 3.5, bootSE = True)\n",
      "    ... result['returnValue']\n",
      "    ... result['se_returnValue']       # return value standard error (asymptotic)\n",
      "    ... result['se_returnValue_boot']  # return value standard error (bootstrapping)\n",
      "    ... result['logReturnProb']        # log of probability of exceeding 'returnValue'\n",
      "    ... result['mle']                  # MLE array \n",
      "    ... result['mle_names']            # names for MLE array \n",
      "    ... result['mle'][2]               # MLE for shape parameter\n",
      "    ... \n",
      "    ... result['numBootFailures']      # number of bootstrap datasets for which the model could not be fit; if this is non-negligible relative to the number of bootstrap samples (default of 250), interpret the bootstrap results with caution\n",
      "    ... \n",
      "    ... # nonstationary fit with location linear in year and two return values requested\n",
      "    ... result = climextremes.fit_pot(numpy.array(FortExc.Prec), x = yrs, firstBlock = firstYr, nBlocks = nYrs, threshold = threshold, blockIndex = numpy.array(FortExc.year), locationFun = 1, getParams = True, returnPeriod = numpy.array([20, 30]), returnValue = 3.5, xNew = yrsToPred, bootSE = False)\n",
      "    ... result['returnValue']\n",
      "    ... result['se_returnValue']\n",
      "    ... \n",
      "    ... # fit with location a function of two covariates\n",
      "    ... # here I'll use year and a random vector just to illustrate syntax\n",
      "    ... # make 'x' be a 2-column numpy array, each column a covariate\n",
      "    ... # 'xNew' also needs to have 2 columns, each row is a different set of covariate values\n",
      "    ... tmp = numpy.random.rand(nYrs)\n",
      "    ... covByBlock = numpy.c_[yrs, numpy.random.rand(nYrs)]\n",
      "    ... result = climextremes.fit_pot(numpy.array(FortExc.Prec), x = covByBlock, firstBlock = firstYr, nBlocks = nYrs, threshold = threshold, blockIndex = numpy.array(FortExc.year), locationFun = numpy.array([1,2]), getParams = True, returnPeriod = 20, returnValue = 3.5, xNew = numpy.array([[min(Fort.year), 0], [max(Fort.year), 0]]), bootSE = False) \n",
      "    ... \n",
      "    ... # with declustering (using max of exceedances on contiguous days)\n",
      "    ... result = climextremes.fit_pot(numpy.array(FortExc.Prec), x = yrs, firstBlock = firstYr, nBlocks = nYrs, threshold = threshold, blockIndex = numpy.array(FortExc.year), index = numpy.array(FortExc.obs), locationFun = 1, declustering = \"noruns\", getParams = True, returnPeriod = 20, returnValue = 3.5, xNew = yrsToPred, bootSE = False) \n",
      "    ... result['returnValue']\n",
      "    ... result['se_returnValue']    \n",
      "    ... \n",
      "    ... # with declustering (consider sequential blocks of 5 days and only use the max of any exceedances within a block)\n",
      "    ... result = climextremes.fit_pot(numpy.array(FortExc.Prec), x = yrs, firstBlock = firstYr, nBlocks = nYrs, threshold = threshold, blockIndex = numpy.array(FortExc.year), index = numpy.array(FortExc.obs), locationFun = 1, declustering = 5, getParams = True, returnPeriod = 20, returnValue = 3.5, xNew = yrsToPred, bootSE = False) \n",
      "    ... result['returnValue']\n",
      "    ... result['se_returnValue']    \n",
      "    ... \n",
      "    ... # with replicates; for illustration here, I'll just duplicate the Fort data\n",
      "    ... result = climextremes.fit_pot(numpy.append(numpy.array(FortExc.Prec), numpy.array(FortExc.Prec)), x = yrs, firstBlock = firstYr, nBlocks = nYrs, nReplicates = 2, threshold = threshold, blockIndex = numpy.append(FortExc.year, FortExc.year), locationFun = 1, getParams = True, returnPeriod = 20, returnValue = 3.5, xNew = yrsToPred, bootSE = False) \n",
      "    ... result['returnValue']\n",
      "    ... result['se_returnValue']    \n",
      "    ... \n",
      "    ... # analysis of seasonal total precipitation\n",
      "    ... tmp = Fort[numpy.logical_and(Fort['month'] < 9, Fort['month'] > 5)]\n",
      "    ... FortSummerTotal = tmp.groupby('year').sum()[['Prec']]\n",
      "    ... FortSummerTotal.reset_index(inplace=True)\n",
      "    ... threshold = numpy.percentile(FortSummerTotal.Prec, 80)\n",
      "    ... FortSummerTotalExc = FortSummerTotal[FortSummerTotal.Prec > threshold]\n",
      "    ... \n",
      "    ... result = climextremes.fit_pot(numpy.array(FortSummerTotalExc.Prec), x = yrs, firstBlock = firstYr, nBlocks = nYrs, blockIndex = numpy.array(FortSummerTotalExc.year), locationFun = 1, threshold = threshold, getParams = True, returnPeriod = 20, returnValue = 10, xNew = yrsToPred, bootSE = False)\n",
      "    ... result['returnValue']\n",
      "    ... result['se_returnValue']    \n",
      "    ... \n",
      "    ... # modifying control arguments and seeing more information on the optimization \n",
      "    ... result = climextremes.fit_pot(numpy.array(FortSummerTotalExc.Prec), x = yrs, firstBlock = firstYr, nBlocks = nYrs, blockIndex = numpy.array(FortSummerTotalExc.year), locationFun = 1, threshold = threshold, getParams = True, returnPeriod = 20, returnValue = 10, xNew = yrsToPred, bootSE = True, bootControl = {'n':150, 'seed':3}, getFit = True)\n",
      "    ... result['info']   # information on the optimization\n",
      "    ... result['info']['counts']  # number of evaluations in the optimization\n",
      "    ... result['info']['counts_names'] # names to interpret 'counts'\n",
      "    ... result['numBootFailures']      # number of bootstrap datasets for which the model could not be fit; if this is non-negligible relative to the number of bootstrap samples (default of 250), interpret the bootstrap results with caution\n",
      "    ... \n",
      "    ... # result['fit']  # voluminous output from the R function that does the fitting\n",
      "    ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(climextremes.fit_pot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b767796-7e25-4276-b733-cb2fdc4fff1b",
   "metadata": {},
   "source": [
    "#### 3.10.1 Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6ebf0729-861c-4928-b785-b762be7f1249",
   "metadata": {},
   "outputs": [],
   "source": [
    "firstYr = min(dt.year)\n",
    "\n",
    "yrs = np.array(range(int(firstYr), int(max(dt.year)+1)))\n",
    "nYrs = len(yrs)\n",
    "yrsToPred = np.array([min(dt.year), max(dt.year)])\n",
    "\n",
    "threshold = 0.395\n",
    "\n",
    "dtExc = dt[dt.Prec > threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bd4a49fc-5e1c-4d56-b383-1f87281742e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mle_names': array(['location', 'scale', 'shape'], dtype='<U8'),\n",
       " 'mle': array([1.38342218, 0.53199406, 0.21200434]),\n",
       " 'se_mle_names': array(['location', 'scale', 'shape'], dtype='<U8'),\n",
       " 'se_mle': array([0.04316565, 0.03694468, 0.03841587]),\n",
       " 'nllh': array([-1359.78829279]),\n",
       " 'returnValue': array([3.58421812]),\n",
       " 'se_returnValue': array([0.30909972]),\n",
       " 'logReturnProb': array([-2.91288844]),\n",
       " 'se_logReturnProb': array([0.29211061]),\n",
       " 'logReturnPeriod': array([2.91288844]),\n",
       " 'se_logReturnPeriod': array([0.29211061]),\n",
       " 'info': {'convergence': array([0], dtype=int32),\n",
       "  'counts_names': array(['function', 'gradient'], dtype='<U8'),\n",
       "  'counts': array([82., nan]),\n",
       "  'message': None,\n",
       "  'failure': array([0], dtype=int32)},\n",
       " 'se_mle_boot_names': array(['location', 'scale', 'shape'], dtype='<U8'),\n",
       " 'se_mle_boot': array([0.04882072, 0.03327289, 0.03282756]),\n",
       " 'se_returnValue_boot_names': array(['20'], dtype='<U2'),\n",
       " 'se_returnValue_boot': array([0.26879222]),\n",
       " 'se_logReturnProb_boot_names': array(['3.5'], dtype='<U3'),\n",
       " 'se_logReturnProb_boot': array([0.26483916]),\n",
       " 'se_logReturnPeriod_boot_names': array(['3.5'], dtype='<U3'),\n",
       " 'se_logReturnPeriod_boot': array([0.26483916]),\n",
       " 'numBootFailures': array([0.])}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stationary fit\n",
    "result = climextremes.fit_pot(np.array(dtExc.Prec),\n",
    "                              nBlocks = nYrs,\n",
    "                              threshold = threshold,\n",
    "                              firstBlock = firstYr,\n",
    "                              blockIndex = np.array(dtExc.year),\n",
    "                              getParams = True,\n",
    "                              returnPeriod = 20,\n",
    "                              returnValue = 3.5,\n",
    "                              bootSE = True)\n",
    "#result['returnValue']\n",
    "#result['se_returnValue']       # return value standard error (asymptotic)\n",
    "#result['se_returnValue_boot']  # return value standard error (bootstrapping)\n",
    "#result['logReturnProb']        # log of probability of exceeding 'returnValue'\n",
    "#result['mle']                  # MLE array \n",
    "#result['mle_names']            # names for MLE array \n",
    "#result['mle'][2]               # MLE for shape parameter\n",
    "\n",
    "#result['numBootFailures']      # number of bootstrap datasets for which the model could not be fit; if this is non-negligible relative to the number of bootstrap samples (default of 250), interpret the bootstrap results with caution\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4718555c-60c7-4c65-aaea-e6f0249a953b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mle_names': array(['mu0', 'mu1', 'scale', 'shape'], dtype='<U5'),\n",
       " 'mle': array([1.23967700e+00, 7.39196421e-05, 5.32138136e-01, 2.12033964e-01]),\n",
       " 'se_mle_names': array(['mu0', 'mu1', 'scale', 'shape'], dtype='<U5'),\n",
       " 'se_mle': array([6.65162564e-01, 3.40505990e-04, 3.69097264e-02, 3.83768574e-02]),\n",
       " 'nllh': array([-1359.81350323]),\n",
       " 'returnValue_column_names': array(['20', '30'], dtype='<U2'),\n",
       " 'returnValue': array([[3.58162322, 4.0139832 ],\n",
       "        [3.58894127, 4.02130125]]),\n",
       " 'se_returnValue_column_names': array(['20', '30'], dtype='<U2'),\n",
       " 'se_returnValue': array([[0.30936143, 0.38771691],\n",
       "        [0.30916228, 0.38750477]]),\n",
       " 'logReturnProb': array([-2.91548945, -2.90823513]),\n",
       " 'se_logReturnProb': array([0.2925977 , 0.29157338]),\n",
       " 'logReturnPeriod': array([2.91548945, 2.90823513]),\n",
       " 'se_logReturnPeriod': array([0.2925977 , 0.29157338]),\n",
       " 'info': {'convergence': array([0], dtype=int32),\n",
       "  'counts_names': array(['function', 'gradient'], dtype='<U8'),\n",
       "  'counts': array([103.,  nan]),\n",
       "  'message': None,\n",
       "  'failure': array([0], dtype=int32)}}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nonstationary fit with location linear in year and two return values requested\n",
    "result = climextremes.fit_pot(np.array(dtExc.Prec),\n",
    "                              x = yrs,\n",
    "                              firstBlock = firstYr,\n",
    "                              nBlocks = nYrs,\n",
    "                              threshold = threshold,\n",
    "                              blockIndex = np.array(dtExc.year),\n",
    "                              locationFun = 1,\n",
    "                              getParams = True,\n",
    "                              returnPeriod = np.array([20, 30]),\n",
    "                              returnValue = 3.5,\n",
    "                              xNew = yrsToPred,\n",
    "                              bootSE = False)\n",
    "#result['returnValue']\n",
    "#result['se_returnValue']\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "078ab7ff-0b0e-4eb6-9594-b34384e6714e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mle_names': array(['mu0', 'mu1', 'mu2', 'scale', 'shape'], dtype='<U5'),\n",
       " 'mle': array([1.22105694e+00, 8.14148902e-05, 7.10097452e-03, 5.31562827e-01,\n",
       "        2.11502332e-01]),\n",
       " 'se_mle_names': array(['mu0', 'mu1', 'mu2', 'scale', 'shape'], dtype='<U5'),\n",
       " 'se_mle': array([6.66586001e-01, 3.40913281e-04, 3.63216313e-02, 3.68488246e-02,\n",
       "        3.83571703e-02]),\n",
       " 'nllh': array([-1359.83238256]),\n",
       " 'returnValue': array([3.57294778, 3.58100786]),\n",
       " 'se_returnValue': array([0.30921437, 0.308942  ]),\n",
       " 'logReturnProb': array([-2.92388565, -2.91588696]),\n",
       " 'se_logReturnProb': array([0.29424344, 0.29306114]),\n",
       " 'logReturnPeriod': array([2.92388565, 2.91588696]),\n",
       " 'se_logReturnPeriod': array([0.29424344, 0.29306114]),\n",
       " 'info': {'convergence': array([0], dtype=int32),\n",
       "  'counts_names': array(['function', 'gradient'], dtype='<U8'),\n",
       "  'counts': array([228.,  nan]),\n",
       "  'message': None,\n",
       "  'failure': array([0], dtype=int32)}}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit with location a function of two covariates\n",
    "# here I'll use year and a random vector just to illustrate syntax\n",
    "# make 'x' be a 2-column numpy array, each column a covariate\n",
    "# 'xNew' also needs to have 2 columns, each row is a different set of covariate values\n",
    "\n",
    "tmp = np.random.rand(nYrs)\n",
    "covByBlock = np.c_[yrs, tmp]\n",
    "\n",
    "result = climextremes.fit_pot(np.array(dtExc.Prec),\n",
    "                              x = covByBlock,\n",
    "                              firstBlock = firstYr,\n",
    "                              nBlocks = nYrs,\n",
    "                              threshold = threshold,\n",
    "                              blockIndex = np.array(dtExc.year),\n",
    "                              locationFun = np.array([1,2]),\n",
    "                              getParams = True,\n",
    "                              returnPeriod = 20,\n",
    "                              returnValue = 3.5,\n",
    "                              xNew = np.array([[min(dt.year), 0],\n",
    "                                                  [max(dt.year), 0]]),\n",
    "                              bootSE = False) \n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bb52333f-2428-4fea-b682-12023645a11a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mle_names': array(['mu0', 'mu1', 'scale', 'shape'], dtype='<U5'),\n",
       " 'mle': array([1.28513404e+00, 3.42831126e-05, 5.39456112e-01, 1.98610374e-01]),\n",
       " 'se_mle_names': array(['mu0', 'mu1', 'scale', 'shape'], dtype='<U5'),\n",
       " 'se_mle': array([7.85876387e-01, 4.02478942e-04, 3.72124512e-02, 4.18695338e-02]),\n",
       " 'nllh': array([-926.58941739]),\n",
       " 'returnValue': array([3.53359411, 3.53698814]),\n",
       " 'se_returnValue': array([0.31300369, 0.31263188]),\n",
       " 'logReturnProb': array([-2.96198628, -2.95856569]),\n",
       " 'se_logReturnProb': array([0.30927276, 0.30847387]),\n",
       " 'logReturnPeriod': array([2.96198628, 2.95856569]),\n",
       " 'se_logReturnPeriod': array([0.30927276, 0.30847387]),\n",
       " 'info': {'convergence': array([0], dtype=int32),\n",
       "  'counts_names': array(['function', 'gradient'], dtype='<U8'),\n",
       "  'counts': array([129.,  nan]),\n",
       "  'message': None,\n",
       "  'failure': array([0], dtype=int32)}}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with declustering (using max of exceedances on contiguous days)\n",
    "result = climextremes.fit_pot(np.array(dtExc.Prec),\n",
    "                              x = yrs,\n",
    "                              firstBlock = firstYr,\n",
    "                              nBlocks = nYrs,\n",
    "                              threshold = threshold,\n",
    "                              blockIndex = np.array(dtExc.year),\n",
    "                              index = np.array(dtExc.obs),\n",
    "                              locationFun = 1,\n",
    "                              declustering = \"noruns\",\n",
    "                              getParams = True,\n",
    "                              returnPeriod = 20,\n",
    "                              returnValue = 3.5,\n",
    "                              xNew = yrsToPred,\n",
    "                              bootSE = False) \n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "234292ac-c822-4c5b-8138-568e7286db6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mle_names': array(['mu0', 'mu1', 'scale', 'shape'], dtype='<U5'),\n",
       " 'mle': array([1.24375536e+00, 5.99459024e-05, 5.40287483e-01, 1.87624616e-01]),\n",
       " 'se_mle_names': array(['mu0', 'mu1', 'scale', 'shape'], dtype='<U5'),\n",
       " 'se_mle': array([8.13205324e-01, 4.16558769e-04, 3.67417751e-02, 4.15876253e-02]),\n",
       " 'nllh': array([-893.78269041]),\n",
       " 'returnValue': array([3.50561881, 3.51155345]),\n",
       " 'se_returnValue': array([0.30406885, 0.30387239]),\n",
       " 'logReturnProb': array([-2.98992438, -2.98378395]),\n",
       " 'se_logReturnProb': array([0.31341692, 0.31243972]),\n",
       " 'logReturnPeriod': array([2.98992438, 2.98378395]),\n",
       " 'se_logReturnPeriod': array([0.31341692, 0.31243972]),\n",
       " 'info': {'convergence': array([0], dtype=int32),\n",
       "  'counts_names': array(['function', 'gradient'], dtype='<U8'),\n",
       "  'counts': array([121.,  nan]),\n",
       "  'message': None,\n",
       "  'failure': array([0], dtype=int32)}}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with declustering (consider sequential blocks of 5 days and only use the max of any exceedances within a block)\n",
    "result = climextremes.fit_pot(np.array(dtExc.Prec),\n",
    "                              x = yrs,\n",
    "                              firstBlock = firstYr,\n",
    "                              nBlocks = nYrs,\n",
    "                              threshold = threshold,\n",
    "                              blockIndex = np.array(dtExc.year),\n",
    "                              index = np.array(dtExc.obs),\n",
    "                              locationFun = 1,\n",
    "                              declustering = 5,\n",
    "                              getParams = True,\n",
    "                              returnPeriod = 20,\n",
    "                              returnValue = 3.5,\n",
    "                              xNew = yrsToPred,\n",
    "                              bootSE = False) \n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "76c75bed-49da-47e9-937e-75aba6a63f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mle_names': array(['mu0', 'mu1', 'scale', 'shape'], dtype='<U5'),\n",
       " 'mle': array([1.23454209e+00, 7.64549967e-05, 5.32031842e-01, 2.11957348e-01]),\n",
       " 'se_mle_names': array(['mu0', 'mu1', 'scale', 'shape'], dtype='<U5'),\n",
       " 'se_mle': array([4.70425395e-01, 2.40819713e-04, 2.61140743e-02, 2.71549088e-02]),\n",
       " 'nllh': array([-2719.62721168]),\n",
       " 'returnValue': array([3.58058921, 3.58815826]),\n",
       " 'se_returnValue': array([0.21886443, 0.21871622]),\n",
       " 'logReturnProb': array([-2.91647978, -2.90897472]),\n",
       " 'se_logReturnProb': array([0.20717297, 0.20642049]),\n",
       " 'logReturnPeriod': array([2.91647978, 2.90897472]),\n",
       " 'se_logReturnPeriod': array([0.20717297, 0.20642049]),\n",
       " 'info': {'convergence': array([0], dtype=int32),\n",
       "  'counts_names': array(['function', 'gradient'], dtype='<U8'),\n",
       "  'counts': array([93., nan]),\n",
       "  'message': None,\n",
       "  'failure': array([0], dtype=int32)}}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with replicates; for illustration here, I'll just duplicate the Fort data\n",
    "\n",
    "result = climextremes.fit_pot(np.append(np.array(dtExc.Prec),\n",
    "                                           np.array(dtExc.Prec)),\n",
    "                              x = yrs,\n",
    "                              firstBlock = firstYr,\n",
    "                              nBlocks = nYrs,\n",
    "                              nReplicates = 2,\n",
    "                              threshold = threshold,\n",
    "                              blockIndex = np.append(dtExc.year, dtExc.year),\n",
    "                              locationFun = 1,\n",
    "                              getParams = True,\n",
    "                              returnPeriod = 20,\n",
    "                              returnValue = 3.5,\n",
    "                              xNew = yrsToPred,\n",
    "                              bootSE = False) \n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "18246a1d-d661-4c96-b15c-0942337aaf9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.57, 1.52, 0.49, ..., 0.76, 0.63, 0.59])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.append(np.array(dtExc.Prec),np.array(dtExc.Prec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "be37090d-2704-4ebc-b51b-91965e9bab11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mle_names': array(['mu0', 'mu1', 'scale', 'shape'], dtype='<U5'),\n",
       " 'mle': array([-2.12117930e+01,  1.27144187e-02,  1.93021247e+00,  3.95083165e-03]),\n",
       " 'se_mle_names': array(['mu0', 'mu1', 'scale', 'shape'], dtype='<U5'),\n",
       " 'se_mle': array([4.21123765e+01, 2.07169046e-02, 2.04987624e+00, 3.91557428e-01]),\n",
       " 'nllh': array([85.19268517]),\n",
       " 'returnValue': array([8.71248062, 9.97120807]),\n",
       " 'se_returnValue': array([1.10117007, 1.47956781]),\n",
       " 'logReturnProb': array([-3.64183891, -3.01010216]),\n",
       " 'se_logReturnProb': array([0.69348874, 0.73891272]),\n",
       " 'logReturnPeriod': array([3.64183891, 3.01010216]),\n",
       " 'se_logReturnPeriod': array([0.69348874, 0.73891272]),\n",
       " 'info': {'convergence': array([0], dtype=int32),\n",
       "  'counts_names': array(['function', 'gradient'], dtype='<U8'),\n",
       "  'counts': array([175.,  nan]),\n",
       "  'message': None,\n",
       "  'failure': array([0], dtype=int32)}}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# analysis of seasonal total precipitation\n",
    "\n",
    "tmp = dt[np.logical_and(dt['month'] < 9, dt['month'] > 5)]\n",
    "\n",
    "dtSummerTotal = tmp.groupby('year').sum()[['Prec']]\n",
    "dtSummerTotal.reset_index(inplace=True)\n",
    "\n",
    "threshold = np.percentile(dtSummerTotal.Prec, 80)\n",
    "dtSummerTotalExc = dtSummerTotal[dtSummerTotal.Prec > threshold]\n",
    "\n",
    "result = climextremes.fit_pot(np.array(dtSummerTotalExc.Prec),\n",
    "                              x = yrs,\n",
    "                              firstBlock = firstYr,\n",
    "                              nBlocks = nYrs,\n",
    "                              blockIndex = np.array(dtSummerTotalExc.year),\n",
    "                              locationFun = 1,\n",
    "                              threshold = float(threshold),\n",
    "                              getParams = True,\n",
    "                              returnPeriod = 20,\n",
    "                              returnValue = 10,\n",
    "                              xNew = yrsToPred,\n",
    "                              bootSE = False)\n",
    "result  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "74b329bd-c329-4c6b-868e-7a1f3e7fa7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# modifying control arguments and seeing more information on the optimization \n",
    "\n",
    "result = climextremes.fit_pot(np.array(dtSummerTotalExc.Prec),\n",
    "                              x = yrs,\n",
    "                              firstBlock = firstYr,\n",
    "                              nBlocks = nYrs,\n",
    "                              blockIndex = np.array(dtSummerTotalExc.year),\n",
    "                              locationFun = 1,\n",
    "                              threshold = float(threshold),\n",
    "                              getParams = True,\n",
    "                              returnPeriod = 20,\n",
    "                              returnValue = 10,\n",
    "                              xNew = yrsToPred,\n",
    "                              bootSE = True,\n",
    "                              bootControl = {'n':150, 'seed':3},\n",
    "                              getFit = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3db5d4d6-33be-42c3-a65e-735d364fd73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'convergence': array([0], dtype=int32), 'counts_names': array(['function', 'gradient'], dtype='<U8'), 'counts': array([175.,  nan]), 'message': None, 'failure': array([0], dtype=int32)}\n",
      "------------------------------------------------------------\n",
      "[175.  nan]\n",
      "------------------------------------------------------------\n",
      "['function' 'gradient']\n",
      "------------------------------------------------------------\n",
      "[44.]\n",
      "------------------------------------------------------------\n",
      "{'call': Rlang( fevd(x = .y ~ 1, data = xObs, threshold = thresholdObs, location.fun = locationFun,  ), 'data.name': array(['.y ~ 1', 'xObs'], dtype='<U6'), 'weights': array([1.]), 'in.data': array([1], dtype=int32), 'x.fun': None, 'missing.values_dim1_names': array(['9', '19', '22', '24', '31', '33', '42', '46', '50', '52', '62',\n",
      "       '66', '68', '71', '78', '80', '83', '92', '93', '98'], dtype='<U2'), 'missing.values': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "      dtype=int32), 'x': array([ 6.94,  7.92,  7.61, 11.35,  7.95,  6.94,  6.96,  6.82, 10.18,\n",
      "       11.47,  9.91,  8.  ,  8.18,  6.75,  7.38,  7.21,  9.65,  7.14,\n",
      "       10.38, 14.79]), 'cov.data': {'x': array([-0.41919192, -0.31818182, -0.28787879, -0.26767677, -0.1969697 ,\n",
      "       -0.17676768, -0.08585859, -0.04545455, -0.00505051,  0.01515152,\n",
      "        0.11616162,  0.15656566,  0.17676768,  0.20707071,  0.27777778,\n",
      "        0.2979798 ,  0.32828283,  0.41919192,  0.42929293,  0.47979798]), '.y': 'Compute Error'}, 'method': array(['MLE'], dtype='<U3'), 'type': array(['PP'], dtype='<U2'), 'period.basis': array(['year'], dtype='<U4'), 'optim.args': {'method': array(['Nelder-Mead'], dtype='<U11')}, 'par.models': {'threshold': None, 'location': None, 'scale': None, 'log.scale': array([0], dtype=int32), 'shape': None, 'term.names': {'threshold': array([], dtype=float64), 'location': array(['x'], dtype='<U1'), 'scale': array([], dtype=float64), 'shape': array([], dtype=float64)}}, 'const.thresh': array([1], dtype=int32), 'const.loc': array([0], dtype=int32), 'const.scale': array([1], dtype=int32), 'const.shape': array([1], dtype=int32), 'time.units': array(['days'], dtype='<U4'), 'span': array([100.]), 'npy': array([365.25]), 'n': array([20], dtype=int32), 'na.action': array(['na.fail'], dtype='<U7'), 'threshold': array([6.726]), 'rate': array([1.]), 'blocks': {'nBlocks': array([100.]), 'threshold': array([6.726, 6.726, 6.726, 6.726, 6.726, 6.726, 6.726, 6.726, 6.726,\n",
      "       6.726, 6.726, 6.726, 6.726, 6.726, 6.726, 6.726, 6.726, 6.726,\n",
      "       6.726, 6.726, 6.726, 6.726, 6.726, 6.726, 6.726, 6.726, 6.726,\n",
      "       6.726, 6.726, 6.726, 6.726, 6.726, 6.726, 6.726, 6.726, 6.726,\n",
      "       6.726, 6.726, 6.726, 6.726, 6.726, 6.726, 6.726, 6.726, 6.726,\n",
      "       6.726, 6.726, 6.726, 6.726, 6.726, 6.726, 6.726, 6.726, 6.726,\n",
      "       6.726, 6.726, 6.726, 6.726, 6.726, 6.726, 6.726, 6.726, 6.726,\n",
      "       6.726, 6.726, 6.726, 6.726, 6.726, 6.726, 6.726, 6.726, 6.726,\n",
      "       6.726, 6.726, 6.726, 6.726, 6.726, 6.726, 6.726, 6.726, 6.726,\n",
      "       6.726, 6.726, 6.726, 6.726, 6.726, 6.726, 6.726, 6.726, 6.726,\n",
      "       6.726, 6.726, 6.726, 6.726, 6.726, 6.726, 6.726, 6.726, 6.726,\n",
      "       6.726]), 'weights': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'data': {'x': array([-0.5       , -0.48989899, -0.47979798, -0.46969697, -0.45959596,\n",
      "       -0.44949495, -0.43939394, -0.42929293, -0.41919192, -0.40909091,\n",
      "       -0.3989899 , -0.38888889, -0.37878788, -0.36868687, -0.35858586,\n",
      "       -0.34848485, -0.33838384, -0.32828283, -0.31818182, -0.30808081,\n",
      "       -0.2979798 , -0.28787879, -0.27777778, -0.26767677, -0.25757576,\n",
      "       -0.24747475, -0.23737374, -0.22727273, -0.21717172, -0.20707071,\n",
      "       -0.1969697 , -0.18686869, -0.17676768, -0.16666667, -0.15656566,\n",
      "       -0.14646465, -0.13636364, -0.12626263, -0.11616162, -0.10606061,\n",
      "       -0.0959596 , -0.08585859, -0.07575758, -0.06565657, -0.05555556,\n",
      "       -0.04545455, -0.03535354, -0.02525253, -0.01515152, -0.00505051,\n",
      "        0.00505051,  0.01515152,  0.02525253,  0.03535354,  0.04545455,\n",
      "        0.05555556,  0.06565657,  0.07575758,  0.08585859,  0.0959596 ,\n",
      "        0.10606061,  0.11616162,  0.12626263,  0.13636364,  0.14646465,\n",
      "        0.15656566,  0.16666667,  0.17676768,  0.18686869,  0.1969697 ,\n",
      "        0.20707071,  0.21717172,  0.22727273,  0.23737374,  0.24747475,\n",
      "        0.25757576,  0.26767677,  0.27777778,  0.28787879,  0.2979798 ,\n",
      "        0.30808081,  0.31818182,  0.32828283,  0.33838384,  0.34848485,\n",
      "        0.35858586,  0.36868687,  0.37878788,  0.38888889,  0.3989899 ,\n",
      "        0.40909091,  0.41919192,  0.42929293,  0.43939394,  0.44949495,\n",
      "        0.45959596,  0.46969697,  0.47979798,  0.48989899,  0.5       ])}, 'proportionMissing': array([0.]), 'designs': {'X.loc_row_names': array(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12',\n",
      "       '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23',\n",
      "       '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34',\n",
      "       '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45',\n",
      "       '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56',\n",
      "       '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67',\n",
      "       '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78',\n",
      "       '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89',\n",
      "       '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100'],\n",
      "      dtype='<U3'), 'X.loc_column_names': array(['(Intercept)', 'x'], dtype='<U11'), 'X.loc': array([[ 1.        , -0.5       ],\n",
      "       [ 1.        , -0.48989899],\n",
      "       [ 1.        , -0.47979798],\n",
      "       [ 1.        , -0.46969697],\n",
      "       [ 1.        , -0.45959596],\n",
      "       [ 1.        , -0.44949495],\n",
      "       [ 1.        , -0.43939394],\n",
      "       [ 1.        , -0.42929293],\n",
      "       [ 1.        , -0.41919192],\n",
      "       [ 1.        , -0.40909091],\n",
      "       [ 1.        , -0.3989899 ],\n",
      "       [ 1.        , -0.38888889],\n",
      "       [ 1.        , -0.37878788],\n",
      "       [ 1.        , -0.36868687],\n",
      "       [ 1.        , -0.35858586],\n",
      "       [ 1.        , -0.34848485],\n",
      "       [ 1.        , -0.33838384],\n",
      "       [ 1.        , -0.32828283],\n",
      "       [ 1.        , -0.31818182],\n",
      "       [ 1.        , -0.30808081],\n",
      "       [ 1.        , -0.2979798 ],\n",
      "       [ 1.        , -0.28787879],\n",
      "       [ 1.        , -0.27777778],\n",
      "       [ 1.        , -0.26767677],\n",
      "       [ 1.        , -0.25757576],\n",
      "       [ 1.        , -0.24747475],\n",
      "       [ 1.        , -0.23737374],\n",
      "       [ 1.        , -0.22727273],\n",
      "       [ 1.        , -0.21717172],\n",
      "       [ 1.        , -0.20707071],\n",
      "       [ 1.        , -0.1969697 ],\n",
      "       [ 1.        , -0.18686869],\n",
      "       [ 1.        , -0.17676768],\n",
      "       [ 1.        , -0.16666667],\n",
      "       [ 1.        , -0.15656566],\n",
      "       [ 1.        , -0.14646465],\n",
      "       [ 1.        , -0.13636364],\n",
      "       [ 1.        , -0.12626263],\n",
      "       [ 1.        , -0.11616162],\n",
      "       [ 1.        , -0.10606061],\n",
      "       [ 1.        , -0.0959596 ],\n",
      "       [ 1.        , -0.08585859],\n",
      "       [ 1.        , -0.07575758],\n",
      "       [ 1.        , -0.06565657],\n",
      "       [ 1.        , -0.05555556],\n",
      "       [ 1.        , -0.04545455],\n",
      "       [ 1.        , -0.03535354],\n",
      "       [ 1.        , -0.02525253],\n",
      "       [ 1.        , -0.01515152],\n",
      "       [ 1.        , -0.00505051],\n",
      "       [ 1.        ,  0.00505051],\n",
      "       [ 1.        ,  0.01515152],\n",
      "       [ 1.        ,  0.02525253],\n",
      "       [ 1.        ,  0.03535354],\n",
      "       [ 1.        ,  0.04545455],\n",
      "       [ 1.        ,  0.05555556],\n",
      "       [ 1.        ,  0.06565657],\n",
      "       [ 1.        ,  0.07575758],\n",
      "       [ 1.        ,  0.08585859],\n",
      "       [ 1.        ,  0.0959596 ],\n",
      "       [ 1.        ,  0.10606061],\n",
      "       [ 1.        ,  0.11616162],\n",
      "       [ 1.        ,  0.12626263],\n",
      "       [ 1.        ,  0.13636364],\n",
      "       [ 1.        ,  0.14646465],\n",
      "       [ 1.        ,  0.15656566],\n",
      "       [ 1.        ,  0.16666667],\n",
      "       [ 1.        ,  0.17676768],\n",
      "       [ 1.        ,  0.18686869],\n",
      "       [ 1.        ,  0.1969697 ],\n",
      "       [ 1.        ,  0.20707071],\n",
      "       [ 1.        ,  0.21717172],\n",
      "       [ 1.        ,  0.22727273],\n",
      "       [ 1.        ,  0.23737374],\n",
      "       [ 1.        ,  0.24747475],\n",
      "       [ 1.        ,  0.25757576],\n",
      "       [ 1.        ,  0.26767677],\n",
      "       [ 1.        ,  0.27777778],\n",
      "       [ 1.        ,  0.28787879],\n",
      "       [ 1.        ,  0.2979798 ],\n",
      "       [ 1.        ,  0.30808081],\n",
      "       [ 1.        ,  0.31818182],\n",
      "       [ 1.        ,  0.32828283],\n",
      "       [ 1.        ,  0.33838384],\n",
      "       [ 1.        ,  0.34848485],\n",
      "       [ 1.        ,  0.35858586],\n",
      "       [ 1.        ,  0.36868687],\n",
      "       [ 1.        ,  0.37878788],\n",
      "       [ 1.        ,  0.38888889],\n",
      "       [ 1.        ,  0.3989899 ],\n",
      "       [ 1.        ,  0.40909091],\n",
      "       [ 1.        ,  0.41919192],\n",
      "       [ 1.        ,  0.42929293],\n",
      "       [ 1.        ,  0.43939394],\n",
      "       [ 1.        ,  0.44949495],\n",
      "       [ 1.        ,  0.45959596],\n",
      "       [ 1.        ,  0.46969697],\n",
      "       [ 1.        ,  0.47979798],\n",
      "       [ 1.        ,  0.48989899],\n",
      "       [ 1.        ,  0.5       ]]), 'X.sc': array([[1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.]]), 'X.sh': array([[1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.]])}, 'X.u': array([[1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.]])}, 'parnames': array(['mu0', 'mu1', 'scale', 'shape'], dtype='<U5'), 'results': {'par_names': array(['mu0', 'mu1', 'scale', 'shape'], dtype='<U5'), 'par': array([3.57496622, 1.25872745, 1.93021247, 0.00395083]), 'value': array([85.19268517]), 'counts_names': array(['function', 'gradient'], dtype='<U8'), 'counts': array([175.,  nan]), 'convergence': array([0], dtype=int32), 'message': None, 'hessian_row_names': array(['mu0', 'mu1', 'scale', 'shape'], dtype='<U5'), 'hessian_column_names': array(['mu0', 'mu1', 'scale', 'shape'], dtype='<U5'), 'hessian': array([[5.30137175e+00, 2.92249244e-01, 8.44261221e+00, 1.30883571e+01],\n",
      "       [2.92249244e-01, 4.57450201e-01, 1.75723908e-01, 7.76313099e-01],\n",
      "       [8.44261221e+00, 1.75723908e-01, 1.89584725e+01, 4.80454441e+01],\n",
      "       [1.30883571e+01, 7.76313099e-01, 4.80454441e+01, 1.78213416e+02]]), 'num.pars': {'location': array([2], dtype=int32), 'scale': array([1], dtype=int32), 'shape': array([1], dtype=int32)}}, 'initial.results': {'Lmoments': {'pars_names': array(['location', 'scale', 'shape', 'rate'], dtype='<U8'), 'pars': array([4.81696156, 0.9574613 , 0.25729468, 0.2       ]), 'likelihood': array([85.56177414])}, 'MOM': {'pars_names': array(['location', 'scale', 'shape'], dtype='<U8'), 'pars': array([3.31732049e+00, 2.11793165e+00, 1.00000000e-08]), 'likelihood': array([85.61647134])}, 'PoissonGP': {'pars_names': array(['scale', '', 'scale', 'shape'], dtype='<U5'), 'pars': array([-1.41587404,  0.        ,  4.57401623,  0.12315757]), 'likelihood': array([94.37354392])}}, 'noInt': array([0, 0, 0], dtype=int32)}\n"
     ]
    }
   ],
   "source": [
    "print(result['info'])   # information on the optimization\n",
    "print('---'*20)\n",
    "print(result['info']['counts'])  # number of evaluations in the optimization\n",
    "print('---'*20)\n",
    "print(result['info']['counts_names']) # names to interpret 'counts'\n",
    "print('---'*20)\n",
    "print(result['numBootFailures'])      # number of bootstrap datasets for which the model could not be fit; if this is non-negligible relative to the number of bootstrap samples (default of 250), interpret the bootstrap results with caution\n",
    "print('---'*20)\n",
    "print(result['fit'])  # voluminous output from the R function that does the fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdf810f-661e-492b-8141-caa3bc421c66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
